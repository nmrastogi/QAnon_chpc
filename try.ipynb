{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dotwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The metric system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First AR build. How'd I do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunsmiths were all booked. So I said fuck it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFW you need to visit the wife's son but he go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peace was never an option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226631</th>\n",
       "      <td>We, for supporting Trump, are extremists accor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226632</th>\n",
       "      <td>Biden Quietly Revokes Trump’s Ban on Chinese C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226633</th>\n",
       "      <td>Night is day. Time to wake up!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226634</th>\n",
       "      <td>Big brother is always there to protek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226635</th>\n",
       "      <td>The people complaining about too many George p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226636 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title\n",
       "0                                       The metric system\n",
       "1                             First AR build. How'd I do?\n",
       "2       Gunsmiths were all booked. So I said fuck it, ...\n",
       "3       TFW you need to visit the wife's son but he go...\n",
       "4                               Peace was never an option\n",
       "...                                                   ...\n",
       "226631  We, for supporting Trump, are extremists accor...\n",
       "226632  Biden Quietly Revokes Trump’s Ban on Chinese C...\n",
       "226633                     Night is day. Time to wake up!\n",
       "226634              Big brother is always there to protek\n",
       "226635  The people complaining about too many George p...\n",
       "\n",
       "[226636 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/scratch/general/vast/u1472278/dotwin.csv\" \n",
    "df=pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-10 21:30:34,968 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 313/313 [02:32<00:00,  2.05it/s]\n",
      "2025-03-10 21:33:08,511 - BERTopic - Embedding - Completed ✓\n",
      "2025-03-10 21:33:08,512 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-03-10 21:33:51,012 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-03-10 21:33:51,013 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-03-10 21:33:51,816 - BERTopic - Cluster - Completed ✓\n",
      "2025-03-10 21:33:51,835 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-03-10 21:33:52,366 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Topics:\n",
      "   Topic  Count                             Name  \\\n",
      "0     -1   3886                 -1_the_to_of_and   \n",
      "1      0    168             0_gun_atf_rifle_ammo   \n",
      "2      1    108        1_biden_joe_harris_hunter   \n",
      "3      2     92     2_pelosi_nancy_coup_military   \n",
      "4      3     92        3_antifa_portland_riot_dc   \n",
      "5      4     87      4_white_black_racist_racism   \n",
      "6      5     81  5_twitter_delete_purge_facebook   \n",
      "7      6     73     6_tax_stimulus_000_introduce   \n",
      "8      7     72   7_potus_account_tweets_twitter   \n",
      "9      8     71         8_mods_reddit_banned_mod   \n",
      "\n",
      "                                      Representation  \\\n",
      "0      [the, to, of, and, in, is, this, you, we, it]   \n",
      "1  [gun, atf, rifle, ammo, pistol, guns, grip, br...   \n",
      "2  [biden, joe, harris, hunter, administration, h...   \n",
      "3  [pelosi, nancy, coup, military, rush, nuclear,...   \n",
      "4  [antifa, portland, riot, dc, capitol, was, ant...   \n",
      "5  [white, black, racist, racism, privilege, race...   \n",
      "6  [twitter, delete, purge, facebook, deleted, we...   \n",
      "7  [tax, stimulus, 000, introduce, residents, bil...   \n",
      "8  [potus, account, tweets, twitter, tweet, delet...   \n",
      "9  [mods, reddit, banned, mod, wah, donaldtrump, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [BEN CARSON IS BASED!  Silencing a significant...  \n",
      "1  [Actual Happening - ATF trying to classify pis...  \n",
      "2  [JOE \"I AM THE PARTY\" BIDEN, Trump this, Biden...  \n",
      "3  [Nancy Pelosi attempted to commit a Military C...  \n",
      "4  [Notice how there was no visible Antifa causin...  \n",
      "5  [ZombaeKillz she/her: \"my daughter is a white ...  \n",
      "6  [Delete your Twitter, Delete your Facebook — D...  \n",
      "7  [A California Plan to Chase Away the Rich, The...  \n",
      "8  [Trump Tweets From POTUS Account, Twitter Then...  \n",
      "9  [The majority of remaining r/conspiracy mods a...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 21:33:56,328 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['gun', 'atf', 'rifle', 'ammo', 'pistol', 'guns', 'grip', 'brace', 'firearm', 'shotgun']\n",
      "Topic 1: ['biden', 'joe', 'harris', 'hunter', 'administration', 'hawley', 'he', 'vote', 'press', 'voters']\n",
      "Topic 2: ['pelosi', 'nancy', 'coup', 'military', 'rush', 'nuclear', 'pentagon', 'limbaugh', 'damning', 'recoiling']\n",
      "Topic 3: ['antifa', 'portland', 'riot', 'dc', 'capitol', 'was', 'anti', 'wheeler', 'facial', 'blm']\n",
      "Topic 4: ['white', 'black', 'racist', 'racism', 'privilege', 'race', 'racists', 'supremacy', 'racial', 'school']\n",
      "Topic 5: ['twitter', 'delete', 'purge', 'facebook', 'deleted', 'wearetrump', 'followers', 'trending', 'lost', 'deleting']\n",
      "Topic 6: ['tax', 'stimulus', '000', 'introduce', 'residents', 'bill', 'bank', 'fund', 'taxes', 'dollars']\n",
      "Topic 7: ['potus', 'account', 'tweets', 'twitter', 'tweet', 'deleted', 'trump', 'suspension', 'tuned', 'official']\n",
      "Topic 8: ['mods', 'reddit', 'banned', 'mod', 'wah', 'donaldtrump', 'ban', 'sub', 'thedonald', 'post']\n",
      "Topic 9: ['song', 'music', 'murphy', 'rex', 'babylon', 'playlist', 'cum', 'anthem', 'town', 'pool']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: Load Twitter Data\n",
    "file_path = \"/scratch/general/vast/u1472278/dotwin.csv\"  # Change this to your file path\n",
    "df = pd.read_csv(file_path).head(10000)\n",
    "\n",
    "# Ensure 'title' column exists\n",
    "if \"title\" not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'title' column with tweets.\")\n",
    "\n",
    "tweets = df[\"title\"].astype(str).tolist()  # Convert to list\n",
    "\n",
    "# Step 2: Initialize BERTopic with a Sentence Transformer Model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Lightweight & fast\n",
    "topic_model = BERTopic(embedding_model=embedding_model, verbose=True)\n",
    "\n",
    "# Step 3: Fit BERTopic on Tweets\n",
    "topics, probs = topic_model.fit_transform(tweets)\n",
    "\n",
    "# Step 4: Display Top Topics\n",
    "print(\"\\nTop 10 Topics:\")\n",
    "print(topic_model.get_topic_info().head(10))  # Shows topic frequency\n",
    "\n",
    "# Step 5: Visualize Topics (Optional)\n",
    "topic_model.visualize_barchart(top_n_topics=10)  # Requires matplotlib\n",
    "topic_model.visualize_hierarchy()  # Topic relationships\n",
    "\n",
    "# Step 6: Save the Model for Later Use\n",
    "topic_model.save(\"bertopic_twitter_model\")\n",
    "\n",
    "# ✅ Single loop to print topics (avoiding duplicates)\n",
    "for topic in range(10):  # Adjust range as needed\n",
    "    words = topic_model.get_topic(topic)  # Get words for topic\n",
    "    if words:  # Ensure topic exists\n",
    "        print(f\"Topic {topic}: {[word[0] for word in words]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Downloading hdbscan-0.8.40-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages (from bertopic) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from bertopic) (2.2.3)\n",
      "Collecting plotly>=4.7.0 (from bertopic)\n",
      "  Downloading plotly-6.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting scikit-learn>=0.22.2.post1 (from bertopic)\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from bertopic) (4.66.5)\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting scipy>=1.0 (from hdbscan>=0.8.29->bertopic)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.0 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Collecting narwhals>=1.15.1 (from plotly>=4.7.0->bertopic)\n",
      "  Downloading narwhals-1.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.22.2.post1->bertopic)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading torch-2.6.0-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading pillow-11.1.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.2->umap-learn>=0.5.0->bertopic)\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /uufs/chpc.utah.edu/common/home/u1472278/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n",
      "Downloading hdbscan-0.8.40-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading plotly-6.0.0-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Downloading narwhals-1.29.1-py3-none-any.whl (308 kB)\n",
      "Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading torch-2.6.0-cp39-cp39-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, urllib3, threadpoolctl, sympy, scipy, safetensors, pyyaml, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, narwhals, MarkupSafe, llvmlite, idna, fsspec, filelock, charset-normalizer, certifi, scikit-learn, requests, plotly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, jinja2, pynndescent, nvidia-cusolver-cu12, huggingface-hub, hdbscan, umap-learn, torch, tokenizers, transformers, sentence-transformers, bertopic\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 bertopic-0.16.4 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.17.0 fsspec-2025.3.0 hdbscan-0.8.40 huggingface-hub-0.29.2 idna-3.10 jinja2-3.1.6 llvmlite-0.43.0 mpmath-1.3.0 narwhals-1.29.1 networkx-3.2.1 numba-0.60.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 plotly-6.0.0 pynndescent-0.5.13 pyyaml-6.0.2 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.13.1 sentence-transformers-3.4.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.6.0 transformers-4.49.0 triton-3.2.0 umap-learn-0.5.7 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>HandofBane</td>\n",
       "      <td>Welcome to KotakuInAction.win - Lifeboat Away</td>\n",
       "      <td>Well, looks like we are launching a bit earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>raraara</td>\n",
       "      <td>mods are asleep, post best girls</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>dragonthingy</td>\n",
       "      <td>Are there any other \".win\" sites fleeing reddi...</td>\n",
       "      <td>Question in the title. Since it seems like red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>Konsaki</td>\n",
       "      <td>Welcome to the .Win Family</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>KevO4444</td>\n",
       "      <td>MFW I asked for it and in less than 6 hours yo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227173</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>KuzoKevin</td>\n",
       "      <td>We, for supporting Trump, are extremists accor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227174</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>undine53</td>\n",
       "      <td>Biden Quietly Revokes Trump’s Ban on Chinese C...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227175</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Danleif</td>\n",
       "      <td>Night is day. Time to wake up!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227176</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Awshit1</td>\n",
       "      <td>Big brother is always there to protek</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227177</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>MikeWho</td>\n",
       "      <td>The people complaining about too many George p...</td>\n",
       "      <td>Lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227178 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site        author  \\\n",
       "0       kotakuinaction    HandofBane   \n",
       "1       kotakuinaction       raraara   \n",
       "2       kotakuinaction  dragonthingy   \n",
       "3       kotakuinaction       Konsaki   \n",
       "4       kotakuinaction      KevO4444   \n",
       "...                ...           ...   \n",
       "227173       thedonald     KuzoKevin   \n",
       "227174  greatawakening      undine53   \n",
       "227175       thedonald       Danleif   \n",
       "227176       thedonald       Awshit1   \n",
       "227177  greatawakening       MikeWho   \n",
       "\n",
       "                                                    title  \\\n",
       "0           Welcome to KotakuInAction.win - Lifeboat Away   \n",
       "1                        mods are asleep, post best girls   \n",
       "2       Are there any other \".win\" sites fleeing reddi...   \n",
       "3                              Welcome to the .Win Family   \n",
       "4       MFW I asked for it and in less than 6 hours yo...   \n",
       "...                                                   ...   \n",
       "227173  We, for supporting Trump, are extremists accor...   \n",
       "227174  Biden Quietly Revokes Trump’s Ban on Chinese C...   \n",
       "227175                     Night is day. Time to wake up!   \n",
       "227176              Big brother is always there to protek   \n",
       "227177  The people complaining about too many George p...   \n",
       "\n",
       "                                                  content  \n",
       "0       Well, looks like we are launching a bit earlie...  \n",
       "1                                                     NaN  \n",
       "2       Question in the title. Since it seems like red...  \n",
       "3                                                     NaN  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "227173                                                NaN  \n",
       "227174                                                NaN  \n",
       "227175                                                NaN  \n",
       "227176                                                NaN  \n",
       "227177                                                Lol  \n",
       "\n",
       "[227178 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"datasets/dotwin_posts.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"author\"].dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if rows with links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6773</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>This_Is_TDWin</td>\n",
       "      <td>Patriots, don‘t  fall  for  the  shills. There...</td>\n",
       "      <td>Patriots, don‘t  fall  for  the  shills. There...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>This_Is_TDWin</td>\n",
       "      <td>place to grab an up to date easily scannable o...</td>\n",
       "      <td>place to grab an up to date easily scannable o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12163</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Norma_Bates</td>\n",
       "      <td>🚨🚨⚠️⚠️ WARNING ⚠️⚠️🚨🚨\\r\\nI think they're about...</td>\n",
       "      <td>https://i.maga.host/ZaoGny8.png\\r\\n\\r\\nhttps:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14844</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>QMAJOR</td>\n",
       "      <td>Amazon has just made Parler inoperable by refu...</td>\n",
       "      <td>@Amazon has just made Parler inoperable by ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18365</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>JasperW</td>\n",
       "      <td>As times get crazier, remember:\\r\\n\\r\\n“Theref...</td>\n",
       "      <td>“Therefore let us be grateful for receiving a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21440</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>noraquis</td>\n",
       "      <td>Victory TV Update: What Happened in D.C. with ...</td>\n",
       "      <td>Victory TV Update: What Happened in D.C. with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25644</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>madmallard</td>\n",
       "      <td>https://www.rockthevote.org/ take a look at th...</td>\n",
       "      <td>I stumbled upon this link https://blacklivesma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30943</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Frogleg3</td>\n",
       "      <td>How to backup and archive videos if you have y...</td>\n",
       "      <td>How to backup and archive videos if you have y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34971</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>SR-71A_Blackbird</td>\n",
       "      <td>https://maga.host is down! The bastards are de...</td>\n",
       "      <td>I didn't even get an error message. It just do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37855</th>\n",
       "      <td>ip2always</td>\n",
       "      <td>Grampslife</td>\n",
       "      <td>https://www.youtube.com/watch?v=HcmbXHVM-iI</td>\n",
       "      <td>https://www.youtube.com/watch?v=HcmbXHVM-iI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39024</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>This_Is_TDWin</td>\n",
       "      <td>/qresearch/ - Q Research★ Research and discuss...</td>\n",
       "      <td>/qresearch/ - Q Research★ Research and discuss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44811</th>\n",
       "      <td>ip2always</td>\n",
       "      <td>THETRUTHofYOUTUBE</td>\n",
       "      <td>My GRANDMA Misses the KING of IRL ONLYUSEMEBLA...</td>\n",
       "      <td>https://www.youtube.com/watch?v=7am8sKatGQc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49631</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>WiseOldOwl</td>\n",
       "      <td>https://conventionofstates.com/take_action</td>\n",
       "      <td>You're seeing a LOT of new threads on the Conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51801</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>MajorClark</td>\n",
       "      <td>SOUND OFF!   Call your reps Friends, you MUST ...</td>\n",
       "      <td>Here is a link to every senator and their offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57666</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>minotaurbeach</td>\n",
       "      <td>Telegram surpassed 500 (https://t.me/durov/147...</td>\n",
       "      <td>Telegram surpassed 500 (https://t.me/durov/147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59866</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>dandeman7</td>\n",
       "      <td>A clue about who has real power - from https:/...</td>\n",
       "      <td>This is only an excerpt: I enjoyed reading the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60450</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Talen_Card</td>\n",
       "      <td>Gen. Burger\\r\\n\\r\\nhttps://realrawnews.com/202...</td>\n",
       "      <td>https://realrawnews.com/2021/01/marine-corps-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60688</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>bitpusher</td>\n",
       "      <td>Staying Connected to Information Sources (http...</td>\n",
       "      <td>You might want to sign up for war room emails,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63815</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>armyofpedes4usa</td>\n",
       "      <td>Learned about a messaging tool, https://elemen...</td>\n",
       "      <td>https://element.io/\\r\\n\\r\\narmyofpedes4usa is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65056</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Amurch2</td>\n",
       "      <td>HOLY SHIT  real journalist at CBS.. Go to her ...</td>\n",
       "      <td>https://mobile.twitter.com/CBS_Herridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67316</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Godswillbedone</td>\n",
       "      <td>THIS IS MY LAST POST. STEVE BANNON HAS TOO MUC...</td>\n",
       "      <td>https://youtu.be/WIn-szOnqdU  To all the good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67864</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>PresidentPussyGraber</td>\n",
       "      <td>AI shut down due to \"racism\". Not a surprise t...</td>\n",
       "      <td>https://archive.is/2A3ku\\r\\n\\r\\nLol, the audac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71009</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>RowdyPoopster</td>\n",
       "      <td>Biden Twitter account \"starts from zero\" with ...</td>\n",
       "      <td>https://www.bbc.com/news/technology-55675826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73933</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>SovereignSon</td>\n",
       "      <td>https://math.dartmouth.edu/~doyle/docs/self/se...</td>\n",
       "      <td>What if a man of action and wisdom stopped the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78977</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>LibertyontheRock</td>\n",
       "      <td>Hardore Leftist arrested in... Florida Intende...</td>\n",
       "      <td>https://populist.press/daniel-alan-baker-fbi-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79870</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>theDialecticalPlaya</td>\n",
       "      <td>Florida law enforcement just arrested an Antif...</td>\n",
       "      <td>I have a feeling it's only going to get worse....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85329</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>howdeepdoesitgo</td>\n",
       "      <td>Feel like this is relevant now given the death...</td>\n",
       "      <td>&gt; Red Cross is corrupt and used as a piggy ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91718</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>MajorClark</td>\n",
       "      <td>PEDOGATE: Adam Schiff’s Crime Scene at The Sta...</td>\n",
       "      <td>http://stateofthenation.co/?p=1627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92575</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>tdwinner2020</td>\n",
       "      <td>Sticky request: https://thedonald.win/p/11S0pn...</td>\n",
       "      <td>Needs sticky: https://thedonald.win/p/11S0pnPn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95464</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Trilby</td>\n",
       "      <td>You must watch this video! It's short...\\r\\n\\r...</td>\n",
       "      <td>The whole plot against us in under 1:14 minute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95776</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>WilliamBarrTraitor</td>\n",
       "      <td>Biden's Jumbtron video for 1-20 : https://gtv....</td>\n",
       "      <td>https://gtv.org/video/id=5f94837c7de25667c0fe0c5e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97182</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>swutalk</td>\n",
       "      <td>Is this really Trump's account??   https://t.m...</td>\n",
       "      <td>[https://t.me/Mrdonald_trump/308](https://t.me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97505</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>DebbieK5320</td>\n",
       "      <td>https://rumble.com/vcxaw5-emergency-alert-info...</td>\n",
       "      <td>https://rumble.com/vcxaw5-emergency-alert-info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100731</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>whatrinosare</td>\n",
       "      <td>Is Trump getting advice from neocons? Why is h...</td>\n",
       "      <td>https://media1.s-nbcnews.com/j/newscms/2017_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116988</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>DebfurmNM</td>\n",
       "      <td>Trump inauguration is going to be epic! Biblic...</td>\n",
       "      <td>You know how china loves to show off thier pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119002</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>FlowersByIrene</td>\n",
       "      <td>https://www.fbi.gov/wanted/seeking-info/violen...</td>\n",
       "      <td>Still waiting for the same request for Antifa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124428</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>jerry015</td>\n",
       "      <td>http://antifa.com redirects to WH?</td>\n",
       "      <td>Just click on the link below or copy and paste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125053</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>GunToast</td>\n",
       "      <td>FYI:  http://antifa.com redirects to https://w...</td>\n",
       "      <td>http://antifa.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126440</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>doug2</td>\n",
       "      <td>https://www.kobo.com/us/en/ebook/his-name-was-...</td>\n",
       "      <td>https://www.kobo.com/us/en/ebook/his-name-was-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134252</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>MAGAMAN300</td>\n",
       "      <td>\"Castle Rock\" Entertainment which is owned by ...</td>\n",
       "      <td>I mean be wrong on all of this but it's someth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141554</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>LurkerWill</td>\n",
       "      <td>Please stop reposting from https://t.me/s/True...</td>\n",
       "      <td>**RULES Reminder:**\\n**Q Supporters:\\nThis is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142213</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Marzieattaks</td>\n",
       "      <td>Lin Wood witch hunt and surprise attack\\nhttps...</td>\n",
       "      <td>I learned today that my alma mater Mercer Law ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145132</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>latetotheparty</td>\n",
       "      <td>THIS group is worthy of a donation if what you...</td>\n",
       "      <td>This vital resource needs funds to expand the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147471</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>whataboob</td>\n",
       "      <td>ELECTION FRAUD — by law, ballots are to be kep...</td>\n",
       "      <td>https://hereistheevidence.com/\\nAZ is so close...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158856</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>JAYVEEZ</td>\n",
       "      <td>destroy this channel with downvotes https://ww...</td>\n",
       "      <td>https://www.youtube.com/c/WhiteHouse/videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160916</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>BidenIsAPeeDoughFile</td>\n",
       "      <td>Other people, spending other people's money, o...</td>\n",
       "      <td>https://www.usdebtclock.org/index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161261</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>VirPopulus</td>\n",
       "      <td>Frens, please consider using the **UNCENSORABL...</td>\n",
       "      <td>Frens, please consider using the **UNCENSORABL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172170</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>mikethemarine</td>\n",
       "      <td>this guy is anti self protection and used to w...</td>\n",
       "      <td>Kimber Statement on Montana\\nConstitutional Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173363</th>\n",
       "      <td>gavinmcinnes</td>\n",
       "      <td>HisShadowX</td>\n",
       "      <td>For those not siding with the Rogue Moderator,...</td>\n",
       "      <td>So for those who don’t know Wall Street Bets w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179170</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Ptt2008</td>\n",
       "      <td>We need your help to avoid China Virus vaccine...</td>\n",
       "      <td>Please, help us to avoid China virus vaccines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189109</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Dog_Sniff_Test</td>\n",
       "      <td>The comments in this disinformation agent's vi...</td>\n",
       "      <td>I can't post my new replies there. So I post t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212211</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>CoolArrows69</td>\n",
       "      <td>Lindell's proof\\nhttps://michaeljlindell.com/</td>\n",
       "      <td>Mike Lindell of My Pillow has evidence. Worth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218331</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>AlinaBelle</td>\n",
       "      <td>Excelent podcast today MonkeyBusiness\\nhttps:/...</td>\n",
       "      <td>https://youtu.be/cYAMDYeazSs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222063</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Ottomanthesultan</td>\n",
       "      <td>Alex Jones says the link https://cantcensortru...</td>\n",
       "      <td>https://cantcensortruth.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224498</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>nats1mom</td>\n",
       "      <td>PLEASE VIEW and PASS on!! Unmasked — Have we u...</td>\n",
       "      <td>https://citizenfreepress.com/breaking/unmasked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site                author  \\\n",
       "6773    greatawakening         This_Is_TDWin   \n",
       "7001    greatawakening         This_Is_TDWin   \n",
       "12163        thedonald           Norma_Bates   \n",
       "14844   greatawakening                QMAJOR   \n",
       "18365        thedonald               JasperW   \n",
       "21440        thedonald              noraquis   \n",
       "25644        thedonald            madmallard   \n",
       "30943        thedonald              Frogleg3   \n",
       "34971        thedonald      SR-71A_Blackbird   \n",
       "37855        ip2always            Grampslife   \n",
       "39024   greatawakening         This_Is_TDWin   \n",
       "44811        ip2always     THETRUTHofYOUTUBE   \n",
       "49631        thedonald            WiseOldOwl   \n",
       "51801        thedonald            MajorClark   \n",
       "57666        thedonald         minotaurbeach   \n",
       "59866   greatawakening             dandeman7   \n",
       "60450        thedonald            Talen_Card   \n",
       "60688        thedonald             bitpusher   \n",
       "63815        thedonald       armyofpedes4usa   \n",
       "65056        thedonald               Amurch2   \n",
       "67316        thedonald        Godswillbedone   \n",
       "67864        thedonald  PresidentPussyGraber   \n",
       "71009        thedonald         RowdyPoopster   \n",
       "73933        thedonald          SovereignSon   \n",
       "78977        thedonald      LibertyontheRock   \n",
       "79870        thedonald   theDialecticalPlaya   \n",
       "85329   greatawakening       howdeepdoesitgo   \n",
       "91718        thedonald            MajorClark   \n",
       "92575        thedonald          tdwinner2020   \n",
       "95464        thedonald                Trilby   \n",
       "95776        thedonald    WilliamBarrTraitor   \n",
       "97182        thedonald               swutalk   \n",
       "97505        thedonald           DebbieK5320   \n",
       "100731       thedonald          whatrinosare   \n",
       "116988       thedonald             DebfurmNM   \n",
       "119002       thedonald        FlowersByIrene   \n",
       "124428       thedonald              jerry015   \n",
       "125053       thedonald              GunToast   \n",
       "126440       thedonald                 doug2   \n",
       "134252  greatawakening            MAGAMAN300   \n",
       "141554  greatawakening            LurkerWill   \n",
       "142213       thedonald          Marzieattaks   \n",
       "145132       thedonald        latetotheparty   \n",
       "147471  greatawakening             whataboob   \n",
       "158856       thedonald               JAYVEEZ   \n",
       "160916       thedonald  BidenIsAPeeDoughFile   \n",
       "161261       thedonald            VirPopulus   \n",
       "172170       thedonald         mikethemarine   \n",
       "173363    gavinmcinnes            HisShadowX   \n",
       "179170       thedonald               Ptt2008   \n",
       "189109       thedonald        Dog_Sniff_Test   \n",
       "212211       thedonald          CoolArrows69   \n",
       "218331       thedonald            AlinaBelle   \n",
       "222063       thedonald      Ottomanthesultan   \n",
       "224498       thedonald              nats1mom   \n",
       "\n",
       "                                                    title  \\\n",
       "6773    Patriots, don‘t  fall  for  the  shills. There...   \n",
       "7001    place to grab an up to date easily scannable o...   \n",
       "12163   🚨🚨⚠️⚠️ WARNING ⚠️⚠️🚨🚨\\r\\nI think they're about...   \n",
       "14844   Amazon has just made Parler inoperable by refu...   \n",
       "18365   As times get crazier, remember:\\r\\n\\r\\n“Theref...   \n",
       "21440   Victory TV Update: What Happened in D.C. with ...   \n",
       "25644   https://www.rockthevote.org/ take a look at th...   \n",
       "30943   How to backup and archive videos if you have y...   \n",
       "34971   https://maga.host is down! The bastards are de...   \n",
       "37855         https://www.youtube.com/watch?v=HcmbXHVM-iI   \n",
       "39024   /qresearch/ - Q Research★ Research and discuss...   \n",
       "44811   My GRANDMA Misses the KING of IRL ONLYUSEMEBLA...   \n",
       "49631          https://conventionofstates.com/take_action   \n",
       "51801   SOUND OFF!   Call your reps Friends, you MUST ...   \n",
       "57666   Telegram surpassed 500 (https://t.me/durov/147...   \n",
       "59866   A clue about who has real power - from https:/...   \n",
       "60450   Gen. Burger\\r\\n\\r\\nhttps://realrawnews.com/202...   \n",
       "60688   Staying Connected to Information Sources (http...   \n",
       "63815   Learned about a messaging tool, https://elemen...   \n",
       "65056   HOLY SHIT  real journalist at CBS.. Go to her ...   \n",
       "67316   THIS IS MY LAST POST. STEVE BANNON HAS TOO MUC...   \n",
       "67864   AI shut down due to \"racism\". Not a surprise t...   \n",
       "71009   Biden Twitter account \"starts from zero\" with ...   \n",
       "73933   https://math.dartmouth.edu/~doyle/docs/self/se...   \n",
       "78977   Hardore Leftist arrested in... Florida Intende...   \n",
       "79870   Florida law enforcement just arrested an Antif...   \n",
       "85329   Feel like this is relevant now given the death...   \n",
       "91718   PEDOGATE: Adam Schiff’s Crime Scene at The Sta...   \n",
       "92575   Sticky request: https://thedonald.win/p/11S0pn...   \n",
       "95464   You must watch this video! It's short...\\r\\n\\r...   \n",
       "95776   Biden's Jumbtron video for 1-20 : https://gtv....   \n",
       "97182   Is this really Trump's account??   https://t.m...   \n",
       "97505   https://rumble.com/vcxaw5-emergency-alert-info...   \n",
       "100731  Is Trump getting advice from neocons? Why is h...   \n",
       "116988  Trump inauguration is going to be epic! Biblic...   \n",
       "119002  https://www.fbi.gov/wanted/seeking-info/violen...   \n",
       "124428                 http://antifa.com redirects to WH?   \n",
       "125053  FYI:  http://antifa.com redirects to https://w...   \n",
       "126440  https://www.kobo.com/us/en/ebook/his-name-was-...   \n",
       "134252  \"Castle Rock\" Entertainment which is owned by ...   \n",
       "141554  Please stop reposting from https://t.me/s/True...   \n",
       "142213  Lin Wood witch hunt and surprise attack\\nhttps...   \n",
       "145132  THIS group is worthy of a donation if what you...   \n",
       "147471  ELECTION FRAUD — by law, ballots are to be kep...   \n",
       "158856  destroy this channel with downvotes https://ww...   \n",
       "160916  Other people, spending other people's money, o...   \n",
       "161261  Frens, please consider using the **UNCENSORABL...   \n",
       "172170  this guy is anti self protection and used to w...   \n",
       "173363  For those not siding with the Rogue Moderator,...   \n",
       "179170  We need your help to avoid China Virus vaccine...   \n",
       "189109  The comments in this disinformation agent's vi...   \n",
       "212211      Lindell's proof\\nhttps://michaeljlindell.com/   \n",
       "218331  Excelent podcast today MonkeyBusiness\\nhttps:/...   \n",
       "222063  Alex Jones says the link https://cantcensortru...   \n",
       "224498  PLEASE VIEW and PASS on!! Unmasked — Have we u...   \n",
       "\n",
       "                                                  content  \n",
       "6773    Patriots, don‘t  fall  for  the  shills. There...  \n",
       "7001    place to grab an up to date easily scannable o...  \n",
       "12163   https://i.maga.host/ZaoGny8.png\\r\\n\\r\\nhttps:/...  \n",
       "14844   @Amazon has just made Parler inoperable by ref...  \n",
       "18365   “Therefore let us be grateful for receiving a ...  \n",
       "21440   Victory TV Update: What Happened in D.C. with ...  \n",
       "25644   I stumbled upon this link https://blacklivesma...  \n",
       "30943   How to backup and archive videos if you have y...  \n",
       "34971   I didn't even get an error message. It just do...  \n",
       "37855         https://www.youtube.com/watch?v=HcmbXHVM-iI  \n",
       "39024   /qresearch/ - Q Research★ Research and discuss...  \n",
       "44811         https://www.youtube.com/watch?v=7am8sKatGQc  \n",
       "49631   You're seeing a LOT of new threads on the Conv...  \n",
       "51801   Here is a link to every senator and their offi...  \n",
       "57666   Telegram surpassed 500 (https://t.me/durov/147...  \n",
       "59866   This is only an excerpt: I enjoyed reading the...  \n",
       "60450   https://realrawnews.com/2021/01/marine-corps-r...  \n",
       "60688   You might want to sign up for war room emails,...  \n",
       "63815   https://element.io/\\r\\n\\r\\narmyofpedes4usa is ...  \n",
       "65056             https://mobile.twitter.com/CBS_Herridge  \n",
       "67316   https://youtu.be/WIn-szOnqdU  To all the good ...  \n",
       "67864   https://archive.is/2A3ku\\r\\n\\r\\nLol, the audac...  \n",
       "71009        https://www.bbc.com/news/technology-55675826  \n",
       "73933   What if a man of action and wisdom stopped the...  \n",
       "78977   https://populist.press/daniel-alan-baker-fbi-a...  \n",
       "79870   I have a feeling it's only going to get worse....  \n",
       "85329   > Red Cross is corrupt and used as a piggy ban...  \n",
       "91718                  http://stateofthenation.co/?p=1627  \n",
       "92575   Needs sticky: https://thedonald.win/p/11S0pnPn...  \n",
       "95464   The whole plot against us in under 1:14 minute...  \n",
       "95776   https://gtv.org/video/id=5f94837c7de25667c0fe0c5e  \n",
       "97182   [https://t.me/Mrdonald_trump/308](https://t.me...  \n",
       "97505   https://rumble.com/vcxaw5-emergency-alert-info...  \n",
       "100731  https://media1.s-nbcnews.com/j/newscms/2017_11...  \n",
       "116988  You know how china loves to show off thier pow...  \n",
       "119002  Still waiting for the same request for Antifa ...  \n",
       "124428  Just click on the link below or copy and paste...  \n",
       "125053                                  http://antifa.com  \n",
       "126440  https://www.kobo.com/us/en/ebook/his-name-was-...  \n",
       "134252  I mean be wrong on all of this but it's someth...  \n",
       "141554  **RULES Reminder:**\\n**Q Supporters:\\nThis is ...  \n",
       "142213  I learned today that my alma mater Mercer Law ...  \n",
       "145132  This vital resource needs funds to expand the ...  \n",
       "147471  https://hereistheevidence.com/\\nAZ is so close...  \n",
       "158856        https://www.youtube.com/c/WhiteHouse/videos  \n",
       "160916             https://www.usdebtclock.org/index.html  \n",
       "161261  Frens, please consider using the **UNCENSORABL...  \n",
       "172170  Kimber Statement on Montana\\nConstitutional Ca...  \n",
       "173363  So for those who don’t know Wall Street Bets w...  \n",
       "179170  Please, help us to avoid China virus vaccines ...  \n",
       "189109  I can't post my new replies there. So I post t...  \n",
       "212211  Mike Lindell of My Pillow has evidence. Worth ...  \n",
       "218331                       https://youtu.be/cYAMDYeazSs  \n",
       "222063                       https://cantcensortruth.com/  \n",
       "224498  https://citizenfreepress.com/breaking/unmasked...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Assuming df is your dataframe and 'title' and 'content' are the columns you want to check\n",
    "\n",
    "# Pre-compile the URL regex pattern\n",
    "url_pattern = re.compile(r'http[s]?://\\S+')\n",
    "\n",
    "# Convert 'title' and 'content' columns to strings\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['content'] = df['content'].astype(str)\n",
    "\n",
    "# Function to check if a string contains a link\n",
    "def contains_link(text):\n",
    "    return bool(url_pattern.search(text))\n",
    "\n",
    "# Filter rows where both 'title' and 'content' contain a link\n",
    "df_links_only = df[df['title'].apply(contains_link) & df['content'].apply(contains_link)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_links_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mike Lindell of My Pillow has evidence. Worth a watch. Same thing we already know.\\nhttps://michaeljlindell.com/'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_links_only[\"content\"][212211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_links_only.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking rows with links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>HandofBane</td>\n",
       "      <td>Welcome to KotakuInAction.win - Lifeboat Away</td>\n",
       "      <td>Well, looks like we are launching a bit earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>raraara</td>\n",
       "      <td>mods are asleep, post best girls</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>dragonthingy</td>\n",
       "      <td>Are there any other \".win\" sites fleeing reddi...</td>\n",
       "      <td>Question in the title. Since it seems like red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>Konsaki</td>\n",
       "      <td>Welcome to the .Win Family</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>KevO4444</td>\n",
       "      <td>MFW I asked for it and in less than 6 hours yo...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227171</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>KuzoKevin</td>\n",
       "      <td>We, for supporting Trump, are extremists accor...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227172</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>undine53</td>\n",
       "      <td>Biden Quietly Revokes Trump’s Ban on Chinese C...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227173</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Danleif</td>\n",
       "      <td>Night is day. Time to wake up!</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227174</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Awshit1</td>\n",
       "      <td>Big brother is always there to protek</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227175</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>MikeWho</td>\n",
       "      <td>The people complaining about too many George p...</td>\n",
       "      <td>Lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site        author  \\\n",
       "0       kotakuinaction    HandofBane   \n",
       "1       kotakuinaction       raraara   \n",
       "2       kotakuinaction  dragonthingy   \n",
       "3       kotakuinaction       Konsaki   \n",
       "4       kotakuinaction      KevO4444   \n",
       "...                ...           ...   \n",
       "227171       thedonald     KuzoKevin   \n",
       "227172  greatawakening      undine53   \n",
       "227173       thedonald       Danleif   \n",
       "227174       thedonald       Awshit1   \n",
       "227175  greatawakening       MikeWho   \n",
       "\n",
       "                                                    title  \\\n",
       "0           Welcome to KotakuInAction.win - Lifeboat Away   \n",
       "1                        mods are asleep, post best girls   \n",
       "2       Are there any other \".win\" sites fleeing reddi...   \n",
       "3                              Welcome to the .Win Family   \n",
       "4       MFW I asked for it and in less than 6 hours yo...   \n",
       "...                                                   ...   \n",
       "227171  We, for supporting Trump, are extremists accor...   \n",
       "227172  Biden Quietly Revokes Trump’s Ban on Chinese C...   \n",
       "227173                     Night is day. Time to wake up!   \n",
       "227174              Big brother is always there to protek   \n",
       "227175  The people complaining about too many George p...   \n",
       "\n",
       "                                                  content  \n",
       "0       Well, looks like we are launching a bit earlie...  \n",
       "1                                                     nan  \n",
       "2       Question in the title. Since it seems like red...  \n",
       "3                                                     nan  \n",
       "4                                                     nan  \n",
       "...                                                   ...  \n",
       "227171                                                nan  \n",
       "227172                                                nan  \n",
       "227173                                                nan  \n",
       "227174                                                nan  \n",
       "227175                                                Lol  \n",
       "\n",
       "[227176 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_cleaned\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>HandofBane</td>\n",
       "      <td>Welcome to KotakuInAction.win - Lifeboat Away</td>\n",
       "      <td>Well, looks like we are launching a bit earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>raraara</td>\n",
       "      <td>mods are asleep, post best girls</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>dragonthingy</td>\n",
       "      <td>Are there any other \".win\" sites fleeing reddi...</td>\n",
       "      <td>Question in the title. Since it seems like red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>Konsaki</td>\n",
       "      <td>Welcome to the .Win Family</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>KevO4444</td>\n",
       "      <td>MFW I asked for it and in less than 6 hours yo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219017</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>KuzoKevin</td>\n",
       "      <td>We, for supporting Trump, are extremists accor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219018</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>undine53</td>\n",
       "      <td>Biden Quietly Revokes Trump’s Ban on Chinese C...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219019</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Danleif</td>\n",
       "      <td>Night is day. Time to wake up!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219020</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Awshit1</td>\n",
       "      <td>Big brother is always there to protek</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219021</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>MikeWho</td>\n",
       "      <td>The people complaining about too many George p...</td>\n",
       "      <td>Lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219022 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site        author  \\\n",
       "0       kotakuinaction    HandofBane   \n",
       "1       kotakuinaction       raraara   \n",
       "2       kotakuinaction  dragonthingy   \n",
       "3       kotakuinaction       Konsaki   \n",
       "4       kotakuinaction      KevO4444   \n",
       "...                ...           ...   \n",
       "219017       thedonald     KuzoKevin   \n",
       "219018  greatawakening      undine53   \n",
       "219019       thedonald       Danleif   \n",
       "219020       thedonald       Awshit1   \n",
       "219021  greatawakening       MikeWho   \n",
       "\n",
       "                                                    title  \\\n",
       "0           Welcome to KotakuInAction.win - Lifeboat Away   \n",
       "1                        mods are asleep, post best girls   \n",
       "2       Are there any other \".win\" sites fleeing reddi...   \n",
       "3                              Welcome to the .Win Family   \n",
       "4       MFW I asked for it and in less than 6 hours yo...   \n",
       "...                                                   ...   \n",
       "219017  We, for supporting Trump, are extremists accor...   \n",
       "219018  Biden Quietly Revokes Trump’s Ban on Chinese C...   \n",
       "219019                     Night is day. Time to wake up!   \n",
       "219020              Big brother is always there to protek   \n",
       "219021  The people complaining about too many George p...   \n",
       "\n",
       "                                                  content  \n",
       "0       Well, looks like we are launching a bit earlie...  \n",
       "1                                                     NaN  \n",
       "2       Question in the title. Since it seems like red...  \n",
       "3                                                     NaN  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "219017                                                NaN  \n",
       "219018                                                NaN  \n",
       "219019                                                NaN  \n",
       "219020                                                NaN  \n",
       "219021                                                Lol  \n",
       "\n",
       "[219022 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Remove rows where both 'title' and 'content' are NaN or empty\n",
    "df_cleaned = df.dropna(subset=['title', 'content'], how='all')  # Drop rows only if both are NaN\n",
    "df_cleaned = df_cleaned[(df_cleaned['title'].str.strip() != '') | (df_cleaned['content'].str.strip() != '')]  # Keep rows if either column has text\n",
    "\n",
    "# Step 2: Remove rows with links in both 'title' and 'content' (detecting links using regex)\n",
    "df_cleaned = df_cleaned[~df_cleaned['title'].str.contains(r'http[s]?://\\S+', na=False)]\n",
    "df_cleaned = df_cleaned[~df_cleaned['content'].str.contains(r'http[s]?://\\S+', na=False)]\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>HandofBane</td>\n",
       "      <td>Welcome to KotakuInAction.win - Lifeboat Away</td>\n",
       "      <td>Well, looks like we are launching a bit earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>raraara</td>\n",
       "      <td>mods are asleep, post best girls</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>dragonthingy</td>\n",
       "      <td>Are there any other \".win\" sites fleeing reddi...</td>\n",
       "      <td>Question in the title. Since it seems like red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>Konsaki</td>\n",
       "      <td>Welcome to the .Win Family</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kotakuinaction</td>\n",
       "      <td>KevO4444</td>\n",
       "      <td>MFW I asked for it and in less than 6 hours yo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219017</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>KuzoKevin</td>\n",
       "      <td>We, for supporting Trump, are extremists accor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219018</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>undine53</td>\n",
       "      <td>Biden Quietly Revokes Trump’s Ban on Chinese C...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219019</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Danleif</td>\n",
       "      <td>Night is day. Time to wake up!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219020</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Awshit1</td>\n",
       "      <td>Big brother is always there to protek</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219021</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>MikeWho</td>\n",
       "      <td>The people complaining about too many George p...</td>\n",
       "      <td>Lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219022 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site        author  \\\n",
       "0       kotakuinaction    HandofBane   \n",
       "1       kotakuinaction       raraara   \n",
       "2       kotakuinaction  dragonthingy   \n",
       "3       kotakuinaction       Konsaki   \n",
       "4       kotakuinaction      KevO4444   \n",
       "...                ...           ...   \n",
       "219017       thedonald     KuzoKevin   \n",
       "219018  greatawakening      undine53   \n",
       "219019       thedonald       Danleif   \n",
       "219020       thedonald       Awshit1   \n",
       "219021  greatawakening       MikeWho   \n",
       "\n",
       "                                                    title  \\\n",
       "0           Welcome to KotakuInAction.win - Lifeboat Away   \n",
       "1                        mods are asleep, post best girls   \n",
       "2       Are there any other \".win\" sites fleeing reddi...   \n",
       "3                              Welcome to the .Win Family   \n",
       "4       MFW I asked for it and in less than 6 hours yo...   \n",
       "...                                                   ...   \n",
       "219017  We, for supporting Trump, are extremists accor...   \n",
       "219018  Biden Quietly Revokes Trump’s Ban on Chinese C...   \n",
       "219019                     Night is day. Time to wake up!   \n",
       "219020              Big brother is always there to protek   \n",
       "219021  The people complaining about too many George p...   \n",
       "\n",
       "                                                  content  \n",
       "0       Well, looks like we are launching a bit earlie...  \n",
       "1                                                     NaN  \n",
       "2       Question in the title. Since it seems like red...  \n",
       "3                                                     NaN  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "219017                                                NaN  \n",
       "219018                                                NaN  \n",
       "219019                                                NaN  \n",
       "219020                                                NaN  \n",
       "219021                                                Lol  \n",
       "\n",
       "[219022 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_cleaned\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m category_counts\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Clean the text in the columns\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Count occurrences of each category in the cleaned text\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[0;32m---> 60\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[1;32m     61\u001b[0m     cleaned_words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m nltk_stopwords]\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_words)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Sample dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dictionaries\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "# Add other dictionaries if needed, similar to the ones above\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "\n",
    "# Function to clean text: remove stop words and lower the text\n",
    "def clean_text(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [re.compile(rf'\\b{re.escape(word)}\\b') for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to count occurrences from each dictionary in the text\n",
    "def count_categories(text):\n",
    "    category_counts = {category: 0 for category in dictionaries}  # Initialize counts\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                category_counts[category] += 1\n",
    "    return category_counts\n",
    "\n",
    "# Clean the text in the columns\n",
    "df['title_cleaned'] = df['title'].apply(clean_text)\n",
    "df['content_cleaned'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Count occurrences of each category in the cleaned text\n",
    "df['title_category_counts'] = df['title_cleaned'].apply(count_categories)\n",
    "df['content_category_counts'] = df['content_cleaned'].apply(count_categories)\n",
    "\n",
    "# Combine the counts for both title and content columns\n",
    "df['combined_category_counts'] = df.apply(lambda row: {category: row['title_category_counts'][category] + row['content_category_counts'][category]\n",
    "                                                       for category in dictionaries}, axis=1)\n",
    "\n",
    "# Print the final combined category counts for both columns\n",
    "combined_category_counts_total = df['combined_category_counts'].apply(pd.Series).sum()\n",
    "print(\"Total combined category counts for both 'title' and 'content' columns:\")\n",
    "print(combined_category_counts_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined category counts for both 'title' and 'content' columns:\n",
      "fusion                     5353\n",
      "violence                   9259\n",
      "identification1               0\n",
      "identification2               0\n",
      "slurs                      1698\n",
      "demonisation               9930\n",
      "dehumanisation             3682\n",
      "existential_threat         3516\n",
      "conspiracy                12758\n",
      "inevitable_war1            8799\n",
      "inevitable_war2           14590\n",
      "violence_justification     2464\n",
      "martyr                      546\n",
      "violent_role_model1         427\n",
      "violent_role_model2        4259\n",
      "hopelessness1              9383\n",
      "hopelessness2              4692\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean text: remove stop words and lower the text\n",
    "def clean_text(text):\n",
    "    # Convert non-string values to an empty string\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Sample dictionaries (you can add more or modify as needed)\n",
    "# Dictionaries\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "# Add other dictionaries if needed, similar to the ones above\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [re.compile(rf'\\b{re.escape(word)}\\b') for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to count occurrences from each dictionary in the text\n",
    "def count_categories(text):\n",
    "    category_counts = {category: 0 for category in dictionaries}  # Initialize counts\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                category_counts[category] += 1\n",
    "    return category_counts\n",
    "\n",
    "# Sample dataframe\n",
    "\n",
    "# Clean the text in the columns\n",
    "df['title_cleaned'] = df['title'].apply(clean_text)\n",
    "df['content_cleaned'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Count occurrences of each category in the cleaned text\n",
    "df['title_category_counts'] = df['title_cleaned'].apply(count_categories)\n",
    "df['content_category_counts'] = df['content_cleaned'].apply(count_categories)\n",
    "\n",
    "# Combine the counts for both title and content columns\n",
    "df['combined_category_counts'] = df.apply(lambda row: {category: row['title_category_counts'][category] + row['content_category_counts'][category]\n",
    "                                                       for category in dictionaries}, axis=1)\n",
    "\n",
    "# Print the final combined category counts for both columns\n",
    "combined_category_counts_total = df['combined_category_counts'].apply(pd.Series).sum()\n",
    "print(\"Total combined category counts for both 'title' and 'content' columns:\")\n",
    "print(combined_category_counts_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined category counts for both 'title' and 'content' columns:\n",
      "fusion                     5353\n",
      "violence                   9259\n",
      "identification1               0\n",
      "identification2               0\n",
      "slurs                      1698\n",
      "demonisation               9930\n",
      "dehumanisation             3682\n",
      "existential_threat         3516\n",
      "conspiracy                12758\n",
      "inevitable_war1            8799\n",
      "inevitable_war2           14590\n",
      "violence_justification     2464\n",
      "martyr                      546\n",
      "violent_role_model1         427\n",
      "violent_role_model2        4259\n",
      "hopelessness1              9383\n",
      "hopelessness2              4692\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Total combined category counts for both 'title' and 'content' columns:\")\n",
    "print(combined_category_counts_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# total tokens code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens from title and content: 4786085\n",
      "Total combined category counts for both 'title' and 'content' columns:\n",
      "fusion                     5353\n",
      "violence                   9259\n",
      "identification1               0\n",
      "identification2               0\n",
      "slurs                      1698\n",
      "demonisation               9930\n",
      "dehumanisation             3682\n",
      "existential_threat         3516\n",
      "conspiracy                12758\n",
      "inevitable_war1            8799\n",
      "inevitable_war2           14590\n",
      "violence_justification     2464\n",
      "martyr                      546\n",
      "violent_role_model1         427\n",
      "violent_role_model2        4259\n",
      "hopelessness1              9383\n",
      "hopelessness2              4692\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Sample dictionaries (you can add more or modify as needed)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "# Add other dictionaries if needed, similar to the ones above\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [re.compile(rf'\\b{re.escape(word)}\\b') for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to clean text: remove stop words and lower the text\n",
    "def clean_text(text):\n",
    "    # Convert non-string values to an empty string\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Function to count occurrences from each dictionary in the text\n",
    "def count_categories(text):\n",
    "    category_counts = {category: 0 for category in dictionaries}  # Initialize counts\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                category_counts[category] += 1\n",
    "    return category_counts\n",
    "\n",
    "# Function to count the number of tokens (words) in cleaned text\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Sample dataframe\n",
    "\n",
    "\n",
    "# Clean the text in the columns\n",
    "df['title_cleaned'] = df['title'].apply(clean_text)\n",
    "df['content_cleaned'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Count occurrences of each category in the cleaned text\n",
    "df['title_category_counts'] = df['title_cleaned'].apply(count_categories)\n",
    "df['content_category_counts'] = df['content_cleaned'].apply(count_categories)\n",
    "\n",
    "# Combine the counts for both title and content columns\n",
    "df['combined_category_counts'] = df.apply(lambda row: {category: row['title_category_counts'][category] + row['content_category_counts'][category]\n",
    "                                                       for category in dictionaries}, axis=1)\n",
    "\n",
    "# Count the total number of tokens in both title and content\n",
    "df['title_token_count'] = df['title_cleaned'].apply(count_tokens)\n",
    "df['content_token_count'] = df['content_cleaned'].apply(count_tokens)\n",
    "\n",
    "# Calculate the total number of tokens from the entire DataFrame\n",
    "total_tokens = df['title_token_count'].sum() + df['content_token_count'].sum()\n",
    "\n",
    "# Print the total number of tokens\n",
    "print(f\"Total number of tokens from title and content: {total_tokens}\")\n",
    "\n",
    "# Print the final combined category counts for both columns\n",
    "combined_category_counts_total = df['combined_category_counts'].apply(pd.Series).sum()\n",
    "print(\"Total combined category counts for both 'title' and 'content' columns:\")\n",
    "print(combined_category_counts_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matched_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>title</td>\n",
       "      <td>Welcome to the win family! Can we make madden ...</td>\n",
       "      <td>fusion</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>title</td>\n",
       "      <td>This Cancer Again?</td>\n",
       "      <td>dehumanisation</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>title</td>\n",
       "      <td>This Cancer Again?</td>\n",
       "      <td>dehumanisation</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>content</td>\n",
       "      <td>Growing up on the Left Coast as a Conservative...</td>\n",
       "      <td>inevitable_war1</td>\n",
       "      <td>fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>content</td>\n",
       "      <td>I want to be one of the first to welcome all t...</td>\n",
       "      <td>fusion</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_index   column                                               text  \\\n",
       "0          9    title  Welcome to the win family! Can we make madden ...   \n",
       "1         11    title                                 This Cancer Again?   \n",
       "2         12    title                                 This Cancer Again?   \n",
       "3         13  content  Growing up on the Left Coast as a Conservative...   \n",
       "4         19  content  I want to be one of the first to welcome all t...   \n",
       "\n",
       "          category matched_word  \n",
       "0           fusion       family  \n",
       "1   dehumanisation       cancer  \n",
       "2   dehumanisation       cancer  \n",
       "3  inevitable_war1        fight  \n",
       "4           fusion       family  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Assuming df is your dataframe and 'title' and 'content' are the columns you want to clean\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Sample dictionaries (as provided)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "# Add other dictionaries if needed, similar to the ones above\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [(word, re.compile(rf'\\b{re.escape(word)}\\b')) for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to track the category and word matches in the text\n",
    "def find_matches(text):\n",
    "    matches = []\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for word, pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                matches.append((category, word))  # Append both category and word to the list\n",
    "    return matches\n",
    "\n",
    "# Clean text function (optional but improves processing)\n",
    "def clean_text(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the cleaning function to the 'title' and 'content' columns\n",
    "df['title_cleaned'] = df['title'].apply(clean_text)\n",
    "df['content_cleaned'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Prepare an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through the first 50,000 rows (or df.shape[0] for the full DataFrame)\n",
    "for idx, row in df.iterrows():\n",
    "    # Get matches from both title and content\n",
    "    title_matches = find_matches(row['title_cleaned'])\n",
    "    content_matches = find_matches(row['content_cleaned'])\n",
    "    \n",
    "    # If there are matches, add them to the results list\n",
    "    for category, word in title_matches:\n",
    "        results.append({\n",
    "            'row_index': idx,\n",
    "            'column': 'title',\n",
    "            'text': row['title'],\n",
    "            'category': category,\n",
    "            'matched_word': word\n",
    "        })\n",
    "    for category, word in content_matches:\n",
    "        results.append({\n",
    "            'row_index': idx,\n",
    "            'column': 'content',\n",
    "            'text': row['content'],\n",
    "            'category': category,\n",
    "            'matched_word': word\n",
    "        })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display or process the final dataframe\n",
    "results_df.head()  # Display the first few rows of the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matched_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>title</td>\n",
       "      <td>Welcome to the win family! Can we make madden ...</td>\n",
       "      <td>fusion</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>title</td>\n",
       "      <td>This Cancer Again?</td>\n",
       "      <td>dehumanisation</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>title</td>\n",
       "      <td>This Cancer Again?</td>\n",
       "      <td>dehumanisation</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>content</td>\n",
       "      <td>Growing up on the Left Coast as a Conservative...</td>\n",
       "      <td>inevitable_war1</td>\n",
       "      <td>fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>content</td>\n",
       "      <td>I want to be one of the first to welcome all t...</td>\n",
       "      <td>fusion</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50877</th>\n",
       "      <td>56537</td>\n",
       "      <td>content</td>\n",
       "      <td>Here in Australia, (((they))) are saying that ...</td>\n",
       "      <td>violent_role_model2</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50878</th>\n",
       "      <td>56537</td>\n",
       "      <td>content</td>\n",
       "      <td>Here in Australia, (((they))) are saying that ...</td>\n",
       "      <td>hopelessness1</td>\n",
       "      <td>democracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50879</th>\n",
       "      <td>56538</td>\n",
       "      <td>content</td>\n",
       "      <td>Was hearing that they busted a hacker trying t...</td>\n",
       "      <td>demonisation</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50880</th>\n",
       "      <td>56540</td>\n",
       "      <td>content</td>\n",
       "      <td>This man will prove that he was a completely w...</td>\n",
       "      <td>demonisation</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50881</th>\n",
       "      <td>56540</td>\n",
       "      <td>content</td>\n",
       "      <td>This man will prove that he was a completely w...</td>\n",
       "      <td>hopelessness1</td>\n",
       "      <td>democracy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50882 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_index   column                                               text  \\\n",
       "0              9    title  Welcome to the win family! Can we make madden ...   \n",
       "1             11    title                                 This Cancer Again?   \n",
       "2             12    title                                 This Cancer Again?   \n",
       "3             13  content  Growing up on the Left Coast as a Conservative...   \n",
       "4             19  content  I want to be one of the first to welcome all t...   \n",
       "...          ...      ...                                                ...   \n",
       "50877      56537  content  Here in Australia, (((they))) are saying that ...   \n",
       "50878      56537  content  Here in Australia, (((they))) are saying that ...   \n",
       "50879      56538  content  Was hearing that they busted a hacker trying t...   \n",
       "50880      56540  content  This man will prove that he was a completely w...   \n",
       "50881      56540  content  This man will prove that he was a completely w...   \n",
       "\n",
       "                  category matched_word  \n",
       "0                   fusion       family  \n",
       "1           dehumanisation       cancer  \n",
       "2           dehumanisation       cancer  \n",
       "3          inevitable_war1        fight  \n",
       "4                   fusion       family  \n",
       "...                    ...          ...  \n",
       "50877  violent_role_model2      support  \n",
       "50878        hopelessness1    democracy  \n",
       "50879         demonisation       poison  \n",
       "50880         demonisation        enemy  \n",
       "50881        hopelessness1    democracy  \n",
       "\n",
       "[50882 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matched_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>823</td>\n",
       "      <td>content</td>\n",
       "      <td>* Q: Is Covid really that deadly as they say? ...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>sacrifice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>930</td>\n",
       "      <td>content</td>\n",
       "      <td>We all know George Floyd is a known criminal w...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>martyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>934</td>\n",
       "      <td>content</td>\n",
       "      <td>I'm losing my faith in Christianity. At the be...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>sacrifice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>998</td>\n",
       "      <td>content</td>\n",
       "      <td>Gun related because if Trump gets impeached he...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>martyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>1134</td>\n",
       "      <td>content</td>\n",
       "      <td>Officer Brian Sicknick died 9:30 pm the next d...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>martyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50397</th>\n",
       "      <td>56086</td>\n",
       "      <td>content</td>\n",
       "      <td>So were officially at our 1st week of doing th...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>sacrifice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50498</th>\n",
       "      <td>56176</td>\n",
       "      <td>content</td>\n",
       "      <td>Tomorrow we will see the guns brought to bear ...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>martyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50541</th>\n",
       "      <td>56208</td>\n",
       "      <td>content</td>\n",
       "      <td>Heavenly father we thank you for this exciting...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>sacrifice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50638</th>\n",
       "      <td>56319</td>\n",
       "      <td>content</td>\n",
       "      <td>(They have taken over face-to-face and online ...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>preserve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50838</th>\n",
       "      <td>56516</td>\n",
       "      <td>content</td>\n",
       "      <td>I'm not saying this is true, I'm saying it's p...</td>\n",
       "      <td>martyr</td>\n",
       "      <td>sacrifice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_index   column                                               text  \\\n",
       "590          823  content  * Q: Is Covid really that deadly as they say? ...   \n",
       "676          930  content  We all know George Floyd is a known criminal w...   \n",
       "680          934  content  I'm losing my faith in Christianity. At the be...   \n",
       "726          998  content  Gun related because if Trump gets impeached he...   \n",
       "877         1134  content  Officer Brian Sicknick died 9:30 pm the next d...   \n",
       "...          ...      ...                                                ...   \n",
       "50397      56086  content  So were officially at our 1st week of doing th...   \n",
       "50498      56176  content  Tomorrow we will see the guns brought to bear ...   \n",
       "50541      56208  content  Heavenly father we thank you for this exciting...   \n",
       "50638      56319  content  (They have taken over face-to-face and online ...   \n",
       "50838      56516  content  I'm not saying this is true, I'm saying it's p...   \n",
       "\n",
       "      category matched_word  \n",
       "590     martyr    sacrifice  \n",
       "676     martyr       martyr  \n",
       "680     martyr    sacrifice  \n",
       "726     martyr       martyr  \n",
       "877     martyr       martyr  \n",
       "...        ...          ...  \n",
       "50397   martyr    sacrifice  \n",
       "50498   martyr       martyr  \n",
       "50541   martyr    sacrifice  \n",
       "50638   martyr     preserve  \n",
       "50838   martyr    sacrifice  \n",
       "\n",
       "[326 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=results_df[results_df[\"category\"]==\"martyr\"]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "violence    641\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matched_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>title</td>\n",
       "      <td>Welcome to the win family! Can we make madden ...</td>\n",
       "      <td>fusion</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>title</td>\n",
       "      <td>This Cancer Again?</td>\n",
       "      <td>dehumanisation</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>title</td>\n",
       "      <td>This Cancer Again?</td>\n",
       "      <td>dehumanisation</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>content</td>\n",
       "      <td>Growing up on the Left Coast as a Conservative...</td>\n",
       "      <td>inevitable_war1</td>\n",
       "      <td>fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>content</td>\n",
       "      <td>I want to be one of the first to welcome all t...</td>\n",
       "      <td>fusion</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_index   column                                               text  \\\n",
       "0          9    title  Welcome to the win family! Can we make madden ...   \n",
       "1         11    title                                 This Cancer Again?   \n",
       "2         12    title                                 This Cancer Again?   \n",
       "3         13  content  Growing up on the Left Coast as a Conservative...   \n",
       "4         19  content  I want to be one of the first to welcome all t...   \n",
       "\n",
       "          category matched_word  \n",
       "0           fusion       family  \n",
       "1   dehumanisation       cancer  \n",
       "2   dehumanisation       cancer  \n",
       "3  inevitable_war1        fight  \n",
       "4           fusion       family  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Assuming df is your dataframe and 'title' and 'content' are the columns you want to clean\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Sample dictionaries (updated with phrases)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"kill us\", \"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill our\", \"kill my\", \"running out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [(word, re.compile(rf'\\b{re.escape(word)}\\b')) for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to track the category and word matches in the text\n",
    "def find_matches(text):\n",
    "    matches = []\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for word, pattern in patterns:\n",
    "            # Check for the entire phrase/word first\n",
    "            if pattern.search(text):\n",
    "                matches.append((category, word))  # Append both category and word to the list\n",
    "    return matches\n",
    "\n",
    "# Clean text function (optional but improves processing)\n",
    "def clean_text(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the cleaning function to the 'title' and 'content' columns\n",
    "df['title_cleaned'] = df['title'].apply(clean_text)\n",
    "df['content_cleaned'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Prepare an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through the rows of the dataframe\n",
    "for idx, row in df.iterrows():\n",
    "    # Get matches from both title and content\n",
    "    title_matches = find_matches(row['title_cleaned'])\n",
    "    content_matches = find_matches(row['content_cleaned'])\n",
    "    \n",
    "    # If there are matches, add them to the results list\n",
    "    for category, word in title_matches:\n",
    "        results.append({\n",
    "            'row_index': idx,\n",
    "            'column': 'title',\n",
    "            'text': row['title'],\n",
    "            'category': category,\n",
    "            'matched_word': word\n",
    "        })\n",
    "    for category, word in content_matches:\n",
    "        results.append({\n",
    "            'row_index': idx,\n",
    "            'column': 'content',\n",
    "            'text': row['content'],\n",
    "            'category': category,\n",
    "            'matched_word': word\n",
    "        })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display or process the final dataframe\n",
    "results_df.head()  # Display the first few rows of the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi word testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matched_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>The hero saved the day</td>\n",
       "      <td>violent_role_model2</td>\n",
       "      <td>hero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>content</td>\n",
       "      <td>Breivik was a hero to some</td>\n",
       "      <td>violent_role_model1</td>\n",
       "      <td>breivik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>content</td>\n",
       "      <td>Breivik was a hero to some</td>\n",
       "      <td>violent_role_model2</td>\n",
       "      <td>hero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>title</td>\n",
       "      <td>We must act in self-defense</td>\n",
       "      <td>violence_justification</td>\n",
       "      <td>self-defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>Weapons and firearms will be needed in self-de...</td>\n",
       "      <td>violence_justification</td>\n",
       "      <td>self-defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>title</td>\n",
       "      <td>Kill us all, this is war</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>title</td>\n",
       "      <td>Kill us all, this is war</td>\n",
       "      <td>existential_threat</td>\n",
       "      <td>kill us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>title</td>\n",
       "      <td>Kill us all, this is war</td>\n",
       "      <td>inevitable_war1</td>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>content</td>\n",
       "      <td>Revolution is imminent, we must fight back</td>\n",
       "      <td>violence</td>\n",
       "      <td>must fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>content</td>\n",
       "      <td>Revolution is imminent, we must fight back</td>\n",
       "      <td>violence</td>\n",
       "      <td>revolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>content</td>\n",
       "      <td>Revolution is imminent, we must fight back</td>\n",
       "      <td>inevitable_war1</td>\n",
       "      <td>fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>content</td>\n",
       "      <td>Revolution is imminent, we must fight back</td>\n",
       "      <td>inevitable_war2</td>\n",
       "      <td>imminent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>title</td>\n",
       "      <td>The brothers united for their motherland</td>\n",
       "      <td>fusion</td>\n",
       "      <td>motherland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>content</td>\n",
       "      <td>Family and comrades should always stand together</td>\n",
       "      <td>fusion</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>content</td>\n",
       "      <td>Family and comrades should always stand together</td>\n",
       "      <td>fusion</td>\n",
       "      <td>comrades</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_index   column                                               text  \\\n",
       "0           0    title                             The hero saved the day   \n",
       "1           0  content                         Breivik was a hero to some   \n",
       "2           0  content                         Breivik was a hero to some   \n",
       "3           2    title                        We must act in self-defense   \n",
       "4           2  content  Weapons and firearms will be needed in self-de...   \n",
       "5           3    title                           Kill us all, this is war   \n",
       "6           3    title                           Kill us all, this is war   \n",
       "7           3    title                           Kill us all, this is war   \n",
       "8           3  content         Revolution is imminent, we must fight back   \n",
       "9           3  content         Revolution is imminent, we must fight back   \n",
       "10          3  content         Revolution is imminent, we must fight back   \n",
       "11          3  content         Revolution is imminent, we must fight back   \n",
       "12          4    title           The brothers united for their motherland   \n",
       "13          4  content   Family and comrades should always stand together   \n",
       "14          4  content   Family and comrades should always stand together   \n",
       "\n",
       "                  category  matched_word  \n",
       "0      violent_role_model2          hero  \n",
       "1      violent_role_model1       breivik  \n",
       "2      violent_role_model2          hero  \n",
       "3   violence_justification  self-defense  \n",
       "4   violence_justification  self-defense  \n",
       "5                 violence          kill  \n",
       "6       existential_threat       kill us  \n",
       "7          inevitable_war1           war  \n",
       "8                 violence    must fight  \n",
       "9                 violence    revolution  \n",
       "10         inevitable_war1         fight  \n",
       "11         inevitable_war2      imminent  \n",
       "12                  fusion    motherland  \n",
       "13                  fusion        family  \n",
       "14                  fusion      comrades  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Assuming df is your dataframe and 'title' and 'content' are the columns you want to clean\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Sample dictionaries (updated with phrases)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"kill us\", \"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill our\", \"kill my\", \"running out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [(word, re.compile(rf'\\b{re.escape(word)}\\b')) for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to track the category and word matches in the text\n",
    "def find_matches(text):\n",
    "    matches = []\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for word, pattern in patterns:\n",
    "            # Check for the entire phrase/word first\n",
    "            if pattern.search(text):\n",
    "                matches.append((category, word))  # Append both category and word to the list\n",
    "    return matches\n",
    "\n",
    "# Clean text function (optional but improves processing)\n",
    "def clean_text(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the cleaning function to the 'title' and 'content' columns\n",
    "data = {\n",
    "    'title': [\n",
    "        'The hero saved the day',\n",
    "        'This is an act of preservation',\n",
    "        'We must act in self-defense',\n",
    "        'Kill us all, this is war',\n",
    "        'The brothers united for their motherland'\n",
    "    ],\n",
    "    'content': [\n",
    "        'Breivik was a hero to some',\n",
    "        'Defending our people is an act of preservation',\n",
    "        'Weapons and firearms will be needed in self-defense',\n",
    "        'Revolution is imminent, we must fight back',\n",
    "        'Family and comrades should always stand together'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['title_cleaned'] = df['title'].apply(clean_text)\n",
    "df['content_cleaned'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Prepare an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through the rows of the dataframe\n",
    "for idx, row in df.iterrows():\n",
    "    # Get matches from both title and content\n",
    "    title_matches = find_matches(row['title_cleaned'])\n",
    "    content_matches = find_matches(row['content_cleaned'])\n",
    "    \n",
    "    # If there are matches, add them to the results list\n",
    "    for category, word in title_matches:\n",
    "        results.append({\n",
    "            'row_index': idx,\n",
    "            'column': 'title',\n",
    "            'text': row['title'],\n",
    "            'category': category,\n",
    "            'matched_word': word\n",
    "        })\n",
    "    for category, word in content_matches:\n",
    "        results.append({\n",
    "            'row_index': idx,\n",
    "            'column': 'content',\n",
    "            'text': row['content'],\n",
    "            'category': category,\n",
    "            'matched_word': word\n",
    "        })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display or process the final dataframe\n",
    "results_df  # Display the first few rows of the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>column</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>matched_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>101</td>\n",
       "      <td>content</td>\n",
       "      <td>Paranoid glowy thoughts\\r\\n\\r\\n\\r\\n* **delete ...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>242</td>\n",
       "      <td>title</td>\n",
       "      <td>How do I know this isn't a honeypot trying to ...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>446</td>\n",
       "      <td>content</td>\n",
       "      <td>* Nearly everyone in power/authority (directin...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>520</td>\n",
       "      <td>content</td>\n",
       "      <td>Listen you fantastically retarded mother fucke...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>555</td>\n",
       "      <td>content</td>\n",
       "      <td>\"Gamergate happened because gamers hated women...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50561</th>\n",
       "      <td>56243</td>\n",
       "      <td>content</td>\n",
       "      <td>DO NOT DOWN VOTE ME OR CURSE ME OUT. I want to...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50588</th>\n",
       "      <td>56279</td>\n",
       "      <td>title</td>\n",
       "      <td>FBI Academy Doctrine: Do you hate honest Ameri...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50606</th>\n",
       "      <td>56295</td>\n",
       "      <td>content</td>\n",
       "      <td>Do they honestly think they can make his base ...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50641</th>\n",
       "      <td>56322</td>\n",
       "      <td>content</td>\n",
       "      <td>To all of you hold outs who wish to cure me of...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50775</th>\n",
       "      <td>56445</td>\n",
       "      <td>content</td>\n",
       "      <td>I read somewhere, and now can't find it, proba...</td>\n",
       "      <td>violence</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>641 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_index   column                                               text  \\\n",
       "62           101  content  Paranoid glowy thoughts\\r\\n\\r\\n\\r\\n* **delete ...   \n",
       "143          242    title  How do I know this isn't a honeypot trying to ...   \n",
       "277          446  content  * Nearly everyone in power/authority (directin...   \n",
       "328          520  content  Listen you fantastically retarded mother fucke...   \n",
       "353          555  content  \"Gamergate happened because gamers hated women...   \n",
       "...          ...      ...                                                ...   \n",
       "50561      56243  content  DO NOT DOWN VOTE ME OR CURSE ME OUT. I want to...   \n",
       "50588      56279    title  FBI Academy Doctrine: Do you hate honest Ameri...   \n",
       "50606      56295  content  Do they honestly think they can make his base ...   \n",
       "50641      56322  content  To all of you hold outs who wish to cure me of...   \n",
       "50775      56445  content  I read somewhere, and now can't find it, proba...   \n",
       "\n",
       "       category matched_word  \n",
       "62     violence         kill  \n",
       "143    violence         kill  \n",
       "277    violence         kill  \n",
       "328    violence         kill  \n",
       "353    violence         kill  \n",
       "...         ...          ...  \n",
       "50561  violence         kill  \n",
       "50588  violence         kill  \n",
       "50606  violence         kill  \n",
       "50641  violence         kill  \n",
       "50775  violence         kill  \n",
       "\n",
       "[641 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=results_df[results_df[\"matched_word\"]==\"kill\"]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "violence    641\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>BigIgloo</td>\n",
       "      <td>I guess you gotta sleep with ear protection ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Tubesy</td>\n",
       "      <td>All of my AK brakes wobble.  At this point I t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>HonkNJhonk</td>\n",
       "      <td>Yes, but what kind of meal should I cook for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Sub-Saharan IQ is a hell of a drug.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Blacks are essentially the Jews pets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596660</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>KuzoKevin</td>\n",
       "      <td>Link to archived article - https://archive.vn/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596661</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Boilingsnowflakes4u</td>\n",
       "      <td>They should have already been rounded up and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596662</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>MythArcana</td>\n",
       "      <td>Isn't this exactly what the 1st Amendment was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596663</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>residue69</td>\n",
       "      <td>But underneath all those feathers they are jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596664</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>JenniferJames</td>\n",
       "      <td>MAD.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1596665 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site               author  \\\n",
       "0        weekendgunnit             BigIgloo   \n",
       "1        weekendgunnit               Tubesy   \n",
       "2        weekendgunnit           HonkNJhonk   \n",
       "3        weekendgunnit               migger   \n",
       "4        weekendgunnit               migger   \n",
       "...                ...                  ...   \n",
       "1596660      thedonald            KuzoKevin   \n",
       "1596661      thedonald  Boilingsnowflakes4u   \n",
       "1596662      thedonald           MythArcana   \n",
       "1596663      thedonald            residue69   \n",
       "1596664      thedonald        JenniferJames   \n",
       "\n",
       "                                                   content  \n",
       "0        I guess you gotta sleep with ear protection ar...  \n",
       "1        All of my AK brakes wobble.  At this point I t...  \n",
       "2        Yes, but what kind of meal should I cook for m...  \n",
       "3                      Sub-Saharan IQ is a hell of a drug.  \n",
       "4                    Blacks are essentially the Jews pets.  \n",
       "...                                                    ...  \n",
       "1596660  Link to archived article - https://archive.vn/...  \n",
       "1596661  They should have already been rounded up and p...  \n",
       "1596662  Isn't this exactly what the 1st Amendment was ...  \n",
       "1596663  But underneath all those feathers they are jus...  \n",
       "1596664                                               MAD.  \n",
       "\n",
       "[1596665 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"datasets/dotwin_comments.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>BigIgloo</td>\n",
       "      <td>I guess you gotta sleep with ear protection ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Tubesy</td>\n",
       "      <td>All of my AK brakes wobble.  At this point I t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>HonkNJhonk</td>\n",
       "      <td>Yes, but what kind of meal should I cook for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Sub-Saharan IQ is a hell of a drug.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Blacks are essentially the Jews pets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537567</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Loiuzein</td>\n",
       "      <td>They should all be tarred, feathered, and quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537568</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Boilingsnowflakes4u</td>\n",
       "      <td>They should have already been rounded up and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537569</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>MythArcana</td>\n",
       "      <td>Isn't this exactly what the 1st Amendment was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537570</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>residue69</td>\n",
       "      <td>But underneath all those feathers they are jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537571</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>JenniferJames</td>\n",
       "      <td>MAD.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1537572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site               author  \\\n",
       "0        weekendgunnit             BigIgloo   \n",
       "1        weekendgunnit               Tubesy   \n",
       "2        weekendgunnit           HonkNJhonk   \n",
       "3        weekendgunnit               migger   \n",
       "4        weekendgunnit               migger   \n",
       "...                ...                  ...   \n",
       "1537567      thedonald             Loiuzein   \n",
       "1537568      thedonald  Boilingsnowflakes4u   \n",
       "1537569      thedonald           MythArcana   \n",
       "1537570      thedonald            residue69   \n",
       "1537571      thedonald        JenniferJames   \n",
       "\n",
       "                                                   content  \n",
       "0        I guess you gotta sleep with ear protection ar...  \n",
       "1        All of my AK brakes wobble.  At this point I t...  \n",
       "2        Yes, but what kind of meal should I cook for m...  \n",
       "3                      Sub-Saharan IQ is a hell of a drug.  \n",
       "4                    Blacks are essentially the Jews pets.  \n",
       "...                                                    ...  \n",
       "1537567  They should all be tarred, feathered, and quar...  \n",
       "1537568  They should have already been rounded up and p...  \n",
       "1537569  Isn't this exactly what the 1st Amendment was ...  \n",
       "1537570  But underneath all those feathers they are jus...  \n",
       "1537571                                               MAD.  \n",
       "\n",
       "[1537572 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Remove rows where both 'title' and 'content' are NaN or empty\n",
    "df_cleaned = df.dropna(subset=['content'], how='all')  # Drop rows only if both are NaN\n",
    "df_cleaned = df_cleaned[(df_cleaned['content'].str.strip() != '')]  # Keep rows if either column has text\n",
    "\n",
    "# Step 2: Remove rows with links in both 'title' and 'content' (detecting links using regex)\n",
    "df_cleaned = df_cleaned[~df_cleaned['content'].str.contains(r'http[s]?://\\S+', na=False)]\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# total tokens code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>BigIgloo</td>\n",
       "      <td>I guess you gotta sleep with ear protection ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Tubesy</td>\n",
       "      <td>All of my AK brakes wobble.  At this point I t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>HonkNJhonk</td>\n",
       "      <td>Yes, but what kind of meal should I cook for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Sub-Saharan IQ is a hell of a drug.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Blacks are essentially the Jews pets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537567</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Loiuzein</td>\n",
       "      <td>They should all be tarred, feathered, and quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537568</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Boilingsnowflakes4u</td>\n",
       "      <td>They should have already been rounded up and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537569</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>MythArcana</td>\n",
       "      <td>Isn't this exactly what the 1st Amendment was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537570</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>residue69</td>\n",
       "      <td>But underneath all those feathers they are jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537571</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>JenniferJames</td>\n",
       "      <td>MAD.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1537572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site               author  \\\n",
       "0        weekendgunnit             BigIgloo   \n",
       "1        weekendgunnit               Tubesy   \n",
       "2        weekendgunnit           HonkNJhonk   \n",
       "3        weekendgunnit               migger   \n",
       "4        weekendgunnit               migger   \n",
       "...                ...                  ...   \n",
       "1537567      thedonald             Loiuzein   \n",
       "1537568      thedonald  Boilingsnowflakes4u   \n",
       "1537569      thedonald           MythArcana   \n",
       "1537570      thedonald            residue69   \n",
       "1537571      thedonald        JenniferJames   \n",
       "\n",
       "                                                   content  \n",
       "0        I guess you gotta sleep with ear protection ar...  \n",
       "1        All of my AK brakes wobble.  At this point I t...  \n",
       "2        Yes, but what kind of meal should I cook for m...  \n",
       "3                      Sub-Saharan IQ is a hell of a drug.  \n",
       "4                    Blacks are essentially the Jews pets.  \n",
       "...                                                    ...  \n",
       "1537567  They should all be tarred, feathered, and quar...  \n",
       "1537568  They should have already been rounded up and p...  \n",
       "1537569  Isn't this exactly what the 1st Amendment was ...  \n",
       "1537570  But underneath all those feathers they are jus...  \n",
       "1537571                                               MAD.  \n",
       "\n",
       "[1537572 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_cleaned\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed with 50000 rows.\n",
      "Processing batch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 processed with 50000 rows.\n",
      "Processing batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 processed with 50000 rows.\n",
      "Processing batch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 processed with 50000 rows.\n",
      "Processing batch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 processed with 50000 rows.\n",
      "Processing batch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 processed with 50000 rows.\n",
      "Processing batch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 processed with 50000 rows.\n",
      "Processing batch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 processed with 50000 rows.\n",
      "Processing batch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 processed with 50000 rows.\n",
      "Processing batch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 processed with 50000 rows.\n",
      "Processing batch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 processed with 50000 rows.\n",
      "Processing batch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 processed with 50000 rows.\n",
      "Processing batch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 processed with 50000 rows.\n",
      "Processing batch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14 processed with 50000 rows.\n",
      "Processing batch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 processed with 50000 rows.\n",
      "Processing batch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16 processed with 50000 rows.\n",
      "Processing batch 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 processed with 50000 rows.\n",
      "Processing batch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18 processed with 50000 rows.\n",
      "Processing batch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 processed with 50000 rows.\n",
      "Processing batch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 processed with 50000 rows.\n",
      "Processing batch 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21 processed with 50000 rows.\n",
      "Processing batch 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22 processed with 50000 rows.\n",
      "Processing batch 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1472278/miniconda3/envs/new_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Sample dictionaries (you can add more or modify as needed)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"kill us\", \"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill our\", \"kill my\", \"running out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [re.compile(rf'\\b{re.escape(word)}\\b') for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to clean text: remove stop words and lower the text\n",
    "def clean_text(text):\n",
    "    # Convert non-string values to an empty string\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Function to count occurrences from each dictionary in the text\n",
    "def count_categories(text):\n",
    "    category_counts = {category: 0 for category in dictionaries}  # Initialize counts\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                category_counts[category] += 1\n",
    "    return category_counts\n",
    "\n",
    "# Function to count the number of tokens (words) in cleaned text\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Function to process a chunk of the DataFrame\n",
    "def process_chunk(chunk):\n",
    "    chunk['content_cleaned'] = chunk['content'].apply(clean_text)\n",
    "    chunk['content_category_counts'] = chunk['content_cleaned'].apply(count_categories)\n",
    "    chunk['content_token_count'] = chunk['content_cleaned'].apply(count_tokens)\n",
    "    return chunk\n",
    "\n",
    "# Function to split the DataFrame into chunks and process them in parallel\n",
    "def multiprocess_dataframe(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = cpu_count()  # Use the number of CPUs available\n",
    "    \n",
    "    # Split the DataFrame into smaller chunks for parallel processing\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Use a pool of workers to process the chunks\n",
    "    with Pool(num_partitions) as pool:\n",
    "        df_chunks = pool.map(process_chunk, df_split)\n",
    "    \n",
    "    # Combine all the processed chunks back into a single DataFrame\n",
    "    return pd.concat(df_chunks).reset_index(drop=True)\n",
    "\n",
    "# Function to handle large dataset and process in chunks\n",
    "def process_in_batches(df, batch_size=50000):\n",
    "    total_rows = df.shape[0]\n",
    "    total_batches = (total_rows // batch_size) + (1 if total_rows % batch_size != 0 else 0)\n",
    "    final_results = [] \n",
    "\n",
    "    for i in range(total_batches):\n",
    "        print(f\"Processing batch {i+1}...\")\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        df_batch = df.iloc[start:end]\n",
    "        # Process using multiple processors\n",
    "        df_batch_cleaned = multiprocess_dataframe(df_batch)\n",
    "        # Append results from this batch to final results\n",
    "        final_results.append(df_batch_cleaned)\n",
    "        print(f\"Batch {i+1} processed with {len(df_batch_cleaned)} rows.\")\n",
    "\n",
    "    # Combine all final results into a single DataFrame\n",
    "    return pd.concat(final_results).reset_index(drop=True)\n",
    "\n",
    "# Function to save the final token count and dictionary counts to a .txt file\n",
    "def save_final_summary(df, output_file='final_summary.txt'):\n",
    "    # Total number of tokens\n",
    "    total_tokens = df['content_token_count'].sum()\n",
    "\n",
    "    # Combined dictionary counts\n",
    "    combined_category_counts_total = df['content_category_counts'].apply(pd.Series).sum()\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Total number of tokens from content: {total_tokens}\\n\")\n",
    "        f.write(\"Total combined category counts for content column:\\n\")\n",
    "        f.write(f\"{combined_category_counts_total}\\n\")\n",
    "\n",
    "    print(f\"Final summary saved to {output_file}.\")\n",
    "\n",
    "# Sample dataframe (For demonstration purposes)\n",
    "\n",
    "\n",
    "# Process the first 200k rows (using batch processing)\n",
    "processed_df = process_in_batches(df)\n",
    "\n",
    "# Save final token count and combined dictionary counts to a text file\n",
    "save_final_summary(processed_df, output_file='final_summary.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>BigIgloo</td>\n",
       "      <td>I guess you gotta sleep with ear protection ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Tubesy</td>\n",
       "      <td>All of my AK brakes wobble.  At this point I t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>HonkNJhonk</td>\n",
       "      <td>Yes, but what kind of meal should I cook for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Sub-Saharan IQ is a hell of a drug.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Blacks are essentially the Jews pets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537567</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Loiuzein</td>\n",
       "      <td>They should all be tarred, feathered, and quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537568</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Boilingsnowflakes4u</td>\n",
       "      <td>They should have already been rounded up and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537569</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>MythArcana</td>\n",
       "      <td>Isn't this exactly what the 1st Amendment was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537570</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>residue69</td>\n",
       "      <td>But underneath all those feathers they are jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537571</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>JenniferJames</td>\n",
       "      <td>MAD.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1537572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site               author  \\\n",
       "0        weekendgunnit             BigIgloo   \n",
       "1        weekendgunnit               Tubesy   \n",
       "2        weekendgunnit           HonkNJhonk   \n",
       "3        weekendgunnit               migger   \n",
       "4        weekendgunnit               migger   \n",
       "...                ...                  ...   \n",
       "1537567      thedonald             Loiuzein   \n",
       "1537568      thedonald  Boilingsnowflakes4u   \n",
       "1537569      thedonald           MythArcana   \n",
       "1537570      thedonald            residue69   \n",
       "1537571      thedonald        JenniferJames   \n",
       "\n",
       "                                                   content  \n",
       "0        I guess you gotta sleep with ear protection ar...  \n",
       "1        All of my AK brakes wobble.  At this point I t...  \n",
       "2        Yes, but what kind of meal should I cook for m...  \n",
       "3                      Sub-Saharan IQ is a hell of a drug.  \n",
       "4                    Blacks are essentially the Jews pets.  \n",
       "...                                                    ...  \n",
       "1537567  They should all be tarred, feathered, and quar...  \n",
       "1537568  They should have already been rounded up and p...  \n",
       "1537569  Isn't this exactly what the 1st Amendment was ...  \n",
       "1537570  But underneath all those feathers they are jus...  \n",
       "1537571                                               MAD.  \n",
       "\n",
       "[1537572 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Assuming df is your dataframe and 'text_column' is the name of the column you want to clean\n",
    "\n",
    "# Step 1: Remove NaN values\n",
    "df_cleaned = df.dropna(subset=['content'])\n",
    "\n",
    "# Step 2: Remove empty rows in both columns (rows that are just whitespace)\n",
    "df_cleaned = df_cleaned[(df_cleaned['content'].str.strip() != '')]\n",
    "\n",
    "# Step 3: Remove rows with links in both columns (detecting links using regex)\n",
    "df_cleaned = df_cleaned[~df_cleaned['content'].str.contains(r'http[s]?://\\S+', na=False)]\n",
    "\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>BigIgloo</td>\n",
       "      <td>I guess you gotta sleep with ear protection ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Tubesy</td>\n",
       "      <td>All of my AK brakes wobble.  At this point I t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>HonkNJhonk</td>\n",
       "      <td>Yes, but what kind of meal should I cook for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Sub-Saharan IQ is a hell of a drug.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>migger</td>\n",
       "      <td>Blacks are essentially the Jews pets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537567</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Loiuzein</td>\n",
       "      <td>They should all be tarred, feathered, and quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537568</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Boilingsnowflakes4u</td>\n",
       "      <td>They should have already been rounded up and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537569</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>MythArcana</td>\n",
       "      <td>Isn't this exactly what the 1st Amendment was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537570</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>residue69</td>\n",
       "      <td>But underneath all those feathers they are jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537571</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>JenniferJames</td>\n",
       "      <td>MAD.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1537572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site               author  \\\n",
       "0        weekendgunnit             BigIgloo   \n",
       "1        weekendgunnit               Tubesy   \n",
       "2        weekendgunnit           HonkNJhonk   \n",
       "3        weekendgunnit               migger   \n",
       "4        weekendgunnit               migger   \n",
       "...                ...                  ...   \n",
       "1537567      thedonald             Loiuzein   \n",
       "1537568      thedonald  Boilingsnowflakes4u   \n",
       "1537569      thedonald           MythArcana   \n",
       "1537570      thedonald            residue69   \n",
       "1537571      thedonald        JenniferJames   \n",
       "\n",
       "                                                   content  \n",
       "0        I guess you gotta sleep with ear protection ar...  \n",
       "1        All of my AK brakes wobble.  At this point I t...  \n",
       "2        Yes, but what kind of meal should I cook for m...  \n",
       "3                      Sub-Saharan IQ is a hell of a drug.  \n",
       "4                    Blacks are essentially the Jews pets.  \n",
       "...                                                    ...  \n",
       "1537567  They should all be tarred, feathered, and quar...  \n",
       "1537568  They should have already been rounded up and p...  \n",
       "1537569  Isn't this exactly what the 1st Amendment was ...  \n",
       "1537570  But underneath all those feathers they are jus...  \n",
       "1537571                                               MAD.  \n",
       "\n",
       "[1537572 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_cleaned\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /uufs/chpc.utah.edu/common/home/u1472278/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK English stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Sample dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dictionaries\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "# Add other dictionaries if needed, similar to the ones above\n",
    "\n",
    "# Add all dictionaries to a list for easier processing\n",
    "dictionaries = {\n",
    "    'fusion': fusion,\n",
    "    'violence': violence,\n",
    "    'identification1': identification1,\n",
    "    'identification2': identification2,\n",
    "    'slurs': slurs,\n",
    "    'demonisation': demonisation,\n",
    "    'dehumanisation': dehumanisation,\n",
    "    'existential_threat': existential_threat,\n",
    "    'conspiracy': conspiracy,\n",
    "    'inevitable_war1': inevitable_war1,\n",
    "    'inevitable_war2': inevitable_war2,\n",
    "    'violence_justification': violence_justification,\n",
    "    'martyr': martyr,\n",
    "    'violent_role_model1': violent_role_model1,\n",
    "    'violent_role_model2': violent_role_model2,\n",
    "    'hopelessness1': hopelessness1,\n",
    "    'hopelessness2': hopelessness2\n",
    "}\n",
    "\n",
    "# Function to clean text: remove stop words and lower the text\n",
    "def clean_text(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in nltk_stopwords]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Pre-compile regex patterns for each dictionary to improve performance\n",
    "compiled_patterns = {category: [re.compile(rf'\\b{re.escape(word)}\\b') for word in words_list]\n",
    "                     for category, words_list in dictionaries.items()}\n",
    "\n",
    "# Function to count occurrences from each dictionary in the text\n",
    "def count_categories(text):\n",
    "    category_counts = {category: 0 for category in dictionaries}  # Initialize counts\n",
    "    for category, patterns in compiled_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(text):\n",
    "                category_counts[category] += 1\n",
    "    return category_counts\n",
    "\n",
    "# Clean the text in the columns\n",
    "\n",
    "df['content_cleaned'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Count occurrences of each category in the cleaned text\n",
    "\n",
    "df['content_category_counts'] = df['content_cleaned'].apply(count_categories)\n",
    "\n",
    "# Combine the counts for both title and content columns\n",
    "combined_category_counts_total = df['content_category_counts'].apply(pd.Series).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total combined category counts for both 'title' and 'content' columns:\")\n",
    "print(combined_category_counts_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸RESPECT!🇺🇸🇺🇸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“STOP ALL Testing” Says Top Pathologist &amp; COVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>Traitor traitor traitor alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>Haven't heard this on CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>We were hoping Gina would come over to Parler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>This is sick, Andy. You and congress have the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>I’m in São Paulo Brazil Airport waiting for my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body\n",
       "0       🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸RESPECT!🇺🇸🇺🇸...\n",
       "1                                              🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧👍\n",
       "2       ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...\n",
       "3       “STOP ALL Testing” Says Top Pathologist & COVI...\n",
       "4       SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...\n",
       "...                                                   ...\n",
       "999995                      Traitor traitor traitor alert\n",
       "999996                          Haven't heard this on CNN\n",
       "999997  We were hoping Gina would come over to Parler ...\n",
       "999998  This is sick, Andy. You and congress have the ...\n",
       "999999  I’m in São Paulo Brazil Airport waiting for my...\n",
       "\n",
       "[1000000 rows x 1 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'datasets/parler_posts_comments.csv'\n",
    "\n",
    "# Read only the first 10,000 rows and a specific column (e.g., 'content')\n",
    "df = pd.read_csv(file_path, usecols=['body'], nrows=1000000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas is installed. Version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"Pandas is installed. Version: {pd.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Pandas is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'datasets/parler_posts_comments.csv'\n",
    "\n",
    "# Read only the first 10,000 rows and a specific column (e.g., 'content')\n",
    "df = pd.read_csv(file_path, usecols=['body'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>content</th>\n",
       "      <th>content_no_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸RESPECT!🇺🇸🇺🇸...</td>\n",
       "      <td>🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸RESPECT!🇺🇸🇺🇸...</td>\n",
       "      <td>RESPECT!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧👍</td>\n",
       "      <td>🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧👍</td>\n",
       "      <td>󠁧󠁢󠁥󠁮󠁧󠁿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...</td>\n",
       "      <td>ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...</td>\n",
       "      <td>ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“STOP ALL Testing” Says Top Pathologist &amp; COVI...</td>\n",
       "      <td>“STOP ALL Testing” Says Top Pathologist &amp; COVI...</td>\n",
       "      <td>“STOP ALL Testing” Says Top Pathologist &amp; COVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...</td>\n",
       "      <td>SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...</td>\n",
       "      <td>SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>It was going on then he couldn’t trust anyone ...</td>\n",
       "      <td>It was going on then he couldn’t trust anyone ...</td>\n",
       "      <td>It was going on then he couldn’t trust anyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>I've spent my adult life fighting for Liberty,...</td>\n",
       "      <td>I've spent my adult life fighting for Liberty,...</td>\n",
       "      <td>I've spent my adult life fighting for Liberty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>How has he not declassified everything yet?</td>\n",
       "      <td>How has he not declassified everything yet?</td>\n",
       "      <td>How has he not declassified everything yet?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>Who can sleep with all of this going on?</td>\n",
       "      <td>Who can sleep with all of this going on?</td>\n",
       "      <td>Who can sleep with all of this going on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>Glad to see you here on Parler where free spee...</td>\n",
       "      <td>Glad to see you here on Parler where free spee...</td>\n",
       "      <td>Glad to see you here on Parler where free spee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      body  \\\n",
       "0        🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸RESPECT!🇺🇸🇺🇸...   \n",
       "1                                               🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧👍   \n",
       "2        ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...   \n",
       "3        “STOP ALL Testing” Says Top Pathologist & COVI...   \n",
       "4        SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...   \n",
       "...                                                    ...   \n",
       "9999995  It was going on then he couldn’t trust anyone ...   \n",
       "9999996  I've spent my adult life fighting for Liberty,...   \n",
       "9999997        How has he not declassified everything yet?   \n",
       "9999998           Who can sleep with all of this going on?   \n",
       "9999999  Glad to see you here on Parler where free spee...   \n",
       "\n",
       "                                                   content  \\\n",
       "0        🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸🇺🇸RESPECT!🇺🇸🇺🇸...   \n",
       "1                                               🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧👍   \n",
       "2        ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...   \n",
       "3        “STOP ALL Testing” Says Top Pathologist & COVI...   \n",
       "4        SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...   \n",
       "...                                                    ...   \n",
       "9999995  It was going on then he couldn’t trust anyone ...   \n",
       "9999996  I've spent my adult life fighting for Liberty,...   \n",
       "9999997        How has he not declassified everything yet?   \n",
       "9999998           Who can sleep with all of this going on?   \n",
       "9999999  Glad to see you here on Parler where free spee...   \n",
       "\n",
       "                                         content_no_emojis  \n",
       "0                                                 RESPECT!  \n",
       "1                                                   󠁧󠁢󠁥󠁮󠁧󠁿  \n",
       "2        ALL DO.. I WILL WATCH WHEN THEY GIVE HIM THE L...  \n",
       "3        “STOP ALL Testing” Says Top Pathologist & COVI...  \n",
       "4        SEEING THAT HAPPEN WILL BEGIN A NEW CHAPTER FO...  \n",
       "...                                                    ...  \n",
       "9999995  It was going on then he couldn’t trust anyone ...  \n",
       "9999996  I've spent my adult life fighting for Liberty,...  \n",
       "9999997        How has he not declassified everything yet?  \n",
       "9999998           Who can sleep with all of this going on?  \n",
       "9999999  Glad to see you here on Parler where free spee...  \n",
       "\n",
       "[10000000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample dataframe\n",
    "\n",
    "\n",
    "# Function to remove emojis from text\n",
    "def remove_emojis(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"  # other symbols\n",
    "            u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "            \"]+\", flags=re.UNICODE\n",
    "        )\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    return text  # Return the value unchanged if it's not a string\n",
    "\n",
    "# Ensure the column is string type (converts all to string)\n",
    "df['content'] = df['body'].astype(str)\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "df['content_no_emojis'] = df['body'].apply(remove_emojis)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'datasets/parler_posts_comments.csv'\n",
    "\n",
    "# Function to remove emojis from text\n",
    "def remove_emojis(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"  # other symbols\n",
    "            u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "            \"]+\", flags=re.UNICODE\n",
    "        )\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    return text  # Return the value unchanged if it's not a string\n",
    "\n",
    "# Process the CSV in chunks of 10 million rows\n",
    "chunksize = 10000000\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(file_path, usecols=['body'], chunksize=chunksize):\n",
    "    # Ensure the 'body' column is string type (converts all to string)\n",
    "    chunk['content'] = chunk['body'].astype(str)\n",
    "\n",
    "    # Apply the function to the 'content' column\n",
    "    chunk['content_no_emojis'] = chunk['content'].apply(remove_emojis)\n",
    "\n",
    "    # Append the processed chunk to the list (or write to a file)\n",
    "    processed_chunks.append(chunk)\n",
    "\n",
    "    # Print progress after processing each chunk\n",
    "    print(f\"Processed a chunk of {len(chunk)} rows.\")\n",
    "\n",
    "# Concatenate all chunks into a single dataframe (optional)\n",
    "df_cleaned = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Show the resulting dataframe (or save it to a file)\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# Optionally save the final dataframe to a new CSV file\n",
    "df_cleaned.to_csv('processed_parler_comments.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Assuming df is your dataframe and 'text_column' is the name of the column you want to clean\n",
    "\n",
    "# Step 1: Remove NaN values\n",
    "df_cleaned = df.dropna(subset=['title', 'content'])\n",
    "\n",
    "# Step 2: Remove empty rows in both columns (rows that are just whitespace)\n",
    "df_cleaned = df_cleaned[(df_cleaned['title'].str.strip() != '') & (df_cleaned['content'].str.strip() != '')]\n",
    "\n",
    "# Step 3: Remove rows with links in both columns (detecting links using regex)\n",
    "df_cleaned = df_cleaned[~df_cleaned['title'].str.contains(r'http[s]?://\\S+', na=False)]\n",
    "df_cleaned = df_cleaned[~df_cleaned['content'].str.contains(r'http[s]?://\\S+', na=False)]\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.5.2\n",
      "  latest version: 24.9.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.9.1\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /uufs/chpc.utah.edu/common/home/u1472278/miniconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    blas-1.0                   |              mkl           6 KB\n",
      "    bottleneck-1.3.7           |   py38ha9d4c09_0         125 KB\n",
      "    certifi-2024.8.30          |   py38h06a4308_0         162 KB\n",
      "    intel-openmp-2021.4.0      |    h06a4308_3561         4.2 MB\n",
      "    mkl-2021.4.0               |     h06a4308_640       142.6 MB\n",
      "    mkl-service-2.4.0          |   py38h7f8727e_0          59 KB\n",
      "    mkl_fft-1.3.1              |   py38hd3c417c_0         180 KB\n",
      "    mkl_random-1.2.2           |   py38h51133e4_0         308 KB\n",
      "    numexpr-2.8.4              |   py38he184ba9_0         134 KB\n",
      "    numpy-1.24.3               |   py38h14f4228_0          11 KB\n",
      "    numpy-base-1.24.3          |   py38h31eccc5_0         6.9 MB\n",
      "    pandas-2.0.3               |   py38h1128e8f_0        12.4 MB\n",
      "    python-dateutil-2.9.0post0 |   py38h06a4308_2         279 KB\n",
      "    python-tzdata-2023.3       |     pyhd3eb1b0_0         140 KB\n",
      "    pytz-2024.1                |   py38h06a4308_0         213 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       167.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               pkgs/main/linux-64::blas-1.0-mkl \n",
      "  bottleneck         pkgs/main/linux-64::bottleneck-1.3.7-py38ha9d4c09_0 \n",
      "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.4.0-h06a4308_3561 \n",
      "  mkl                pkgs/main/linux-64::mkl-2021.4.0-h06a4308_640 \n",
      "  mkl-service        pkgs/main/linux-64::mkl-service-2.4.0-py38h7f8727e_0 \n",
      "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.1-py38hd3c417c_0 \n",
      "  mkl_random         pkgs/main/linux-64::mkl_random-1.2.2-py38h51133e4_0 \n",
      "  numexpr            pkgs/main/linux-64::numexpr-2.8.4-py38he184ba9_0 \n",
      "  numpy              pkgs/main/linux-64::numpy-1.24.3-py38h14f4228_0 \n",
      "  numpy-base         pkgs/main/linux-64::numpy-base-1.24.3-py38h31eccc5_0 \n",
      "  pandas             pkgs/main/linux-64::pandas-2.0.3-py38h1128e8f_0 \n",
      "  python-dateutil    pkgs/main/linux-64::python-dateutil-2.9.0post0-py38h06a4308_2 \n",
      "  python-tzdata      pkgs/main/noarch::python-tzdata-2023.3-pyhd3eb1b0_0 \n",
      "  pytz               pkgs/main/linux-64::pytz-2024.1-py38h06a4308_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2023.05.30-h06a4308_0 --> 2024.9.24-h06a4308_0 \n",
      "  certifi                           2023.5.7-py38h06a4308_0 --> 2024.8.30-py38h06a4308_0 \n",
      "  openssl                                  3.0.9-h7f8727e_0 --> 3.0.15-h5eee18b_0 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           tweet_text\n",
      "0   I want President Joe Biden to call back into t...\n",
      "1   @SoozUK @realDonaldTrump Trump Winning....Agai...\n",
      "2   After Trump''s presidency -- whenever that hap...\n",
      "3   Pfizer was smart not to take Trump’s corrupted...\n",
      "4   A CNN commentator said ‘maybe we can get back ...\n",
      "..                                                ...\n",
      "95  @adoolan34 Exactly, both!🤪\\nNo fresh pick by m...\n",
      "96  @LouDobbs @realDonaldTrump Pfizer was not part...\n",
      "97  Here is the hotline for Trump campaign's clear...\n",
      "98  @Reuters @realDonaldTrump https://t.co/jnqs3oE6g9\n",
      "99  @tedlieu @megynkelly Typical CA politician lyi...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path and the column you want to read\n",
    "file_path = '/scratch/general/vast/u1472278/tweets.csv'\n",
    "\n",
    "# Read only a specific column, for example, 'column_name'\n",
    "df = pd.read_csv(file_path, usecols=['tweet_text'],nrows=100)\n",
    "# Display the column\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in CSV (excluding header): 385070092\n"
     ]
    }
   ],
   "source": [
    "def count_csv_rows(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        row_count = sum(1 for row in file) - 1  # Subtract 1 for the header row\n",
    "    return row_count\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/scratch/general/vast/u1472278/tweets.csv\"\n",
    "total_rows = count_csv_rows(file_path)\n",
    "print(f\"Total rows in CSV (excluding header): {total_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 171266910\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def count_rows_in_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        # Skip the header row and count the remaining rows\n",
    "        row_count = sum(1 for row in reader) - 1\n",
    "    return row_count\n",
    "\n",
    "file_path = '/scratch/general/vast/u1472278/tweets.csv'  # Replace with your file path\n",
    "row_count = count_rows_in_csv(file_path)\n",
    "print(f\"Number of rows: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "field larger than field limit (131072)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row_count\n\u001b[1;32m     10\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/uufs/chpc.utah.edu/common/home/u1472278/QAnon/datasets/parler_posts_comments.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your file path\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m row_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_rows_in_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mcount_rows_in_csv\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Skip the header row and count the remaining rows\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     row_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row_count\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Skip the header row and count the remaining rows\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     row_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row_count\n",
      "\u001b[0;31mError\u001b[0m: field larger than field limit (131072)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def count_rows_in_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        # Skip the header row and count the remaining rows\n",
    "        row_count = sum(1 for row in reader) - 1\n",
    "    return row_count\n",
    "\n",
    "file_path = '/uufs/chpc.utah.edu/common/home/u1472278/QAnon/datasets/parler_posts_comments.csv'  # Replace with your file path\n",
    "row_count = count_rows_in_csv(file_path)\n",
    "print(f\"Number of rows: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Counts:\n",
      "    total_tokens   fusion  violence  identification1  identification2   slurs  \\\n",
      "0    1611171897  1994625   1968959                0                0  116251   \n",
      "\n",
      "   demonisation  dehumanisation  existential_threat  conspiracy  \\\n",
      "0       3636134         1098024             1008551     6688445   \n",
      "\n",
      "   inevitable_war1  inevitable_war2  violence_justification  martyr  \\\n",
      "0          1642328          3312205                  739171  134014   \n",
      "\n",
      "   violent_role_model1  violent_role_model2  hopelessness1  hopelessness2  \n",
      "0               118903              1652793        3779498        1372241  \n",
      "\n",
      "Category Percentages by Total Tokens:\n",
      "    fusion  violence  identification1  identification2     slurs  demonisation  \\\n",
      "0  0.1238  0.122207              0.0              0.0  0.007215      0.225683   \n",
      "\n",
      "   dehumanisation  existential_threat  conspiracy  inevitable_war1  \\\n",
      "0        0.068151            0.062597    0.415129         0.101934   \n",
      "\n",
      "   inevitable_war2  violence_justification    martyr  violent_role_model1  \\\n",
      "0         0.205577                0.045878  0.008318              0.00738   \n",
      "\n",
      "   violent_role_model2  hopelessness1  hopelessness2  \n",
      "0             0.102583       0.234581        0.08517  \n",
      "\n",
      "Sum of All Categories: 29262142\n",
      "Percentage of Total Tokens: 1.82%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/uufs/chpc.utah.edu/common/home/u1472278/QAnon/partial_results.txt'\n",
    "\n",
    "# Initialize a dictionary to store cumulative counts for each category and total tokens\n",
    "cumulative_counts = {\n",
    "    \"total_tokens\": 0,\n",
    "    \"fusion\": 0,\n",
    "    \"violence\": 0,\n",
    "    \"identification1\": 0,\n",
    "    \"identification2\": 0,\n",
    "    \"slurs\": 0,\n",
    "    \"demonisation\": 0,\n",
    "    \"dehumanisation\": 0,\n",
    "    \"existential_threat\": 0,\n",
    "    \"conspiracy\": 0,\n",
    "    \"inevitable_war1\": 0,\n",
    "    \"inevitable_war2\": 0,\n",
    "    \"violence_justification\": 0,\n",
    "    \"martyr\": 0,\n",
    "    \"violent_role_model1\": 0,\n",
    "    \"violent_role_model2\": 0,\n",
    "    \"hopelessness1\": 0,\n",
    "    \"hopelessness2\": 0\n",
    "}\n",
    "\n",
    "# Regular expressions to match relevant lines\n",
    "token_pattern = re.compile(r\"Total number of tokens:\\s+(\\d+)\")\n",
    "category_pattern = re.compile(r\"(\\w+)\\s+(\\d+)\")\n",
    "\n",
    "# Parse the file and aggregate counts\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Check for total tokens line\n",
    "        token_match = token_pattern.search(line)\n",
    "        if token_match:\n",
    "            cumulative_counts[\"total_tokens\"] += int(token_match.group(1))\n",
    "        \n",
    "        # Check for category lines\n",
    "        category_match = category_pattern.search(line)\n",
    "        if category_match:\n",
    "            category, count = category_match.groups()\n",
    "            if category in cumulative_counts:\n",
    "                cumulative_counts[category] += int(count)\n",
    "\n",
    "# Calculate the percentage for each category based on total tokens\n",
    "total_tokens = cumulative_counts[\"total_tokens\"]\n",
    "percentages = {category: (count / total_tokens) * 100 for category, count in cumulative_counts.items() if category != \"total_tokens\"}\n",
    "\n",
    "# Calculate the sum of all category counts and its percentage of the total tokens\n",
    "sum_of_all_categories = sum([count for category, count in cumulative_counts.items() if category != \"total_tokens\"])\n",
    "percentage_of_total_tokens = (sum_of_all_categories / total_tokens) * 100\n",
    "\n",
    "# Convert cumulative counts and percentages to DataFrames\n",
    "df_cumulative_counts = pd.DataFrame([cumulative_counts])\n",
    "df_percentages = pd.DataFrame([percentages])\n",
    "\n",
    "# Display the cumulative counts, percentages, and summary of all categories\n",
    "print(\"Aggregated Counts:\\n\", df_cumulative_counts)\n",
    "print(\"\\nCategory Percentages by Total Tokens:\\n\", df_percentages)\n",
    "print(\"\\nSum of All Categories:\", sum_of_all_categories)\n",
    "print(f\"Percentage of Total Tokens: {percentage_of_total_tokens:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parler aggre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Counts:\n",
      "    total_tokens   fusion  violence  identification1  identification2   slurs  \\\n",
      "0     890306968  1215261   1814624                0                0  450038   \n",
      "\n",
      "   demonisation  dehumanisation  existential_threat  conspiracy  \\\n",
      "0       2855425         1127313              599989     3095882   \n",
      "\n",
      "   inevitable_war1  inevitable_war2  violence_justification  martyr  \\\n",
      "0          1463100          1930021                  432656   75158   \n",
      "\n",
      "   violent_role_model1  violent_role_model2  hopelessness1  hopelessness2  \n",
      "0                75490               778320        1473368         707240  \n",
      "\n",
      "Category Percentages by Total Tokens:\n",
      "      fusion  violence  identification1  identification2     slurs  \\\n",
      "0  0.136499   0.20382              0.0              0.0  0.050549   \n",
      "\n",
      "   demonisation  dehumanisation  existential_threat  conspiracy  \\\n",
      "0      0.320724        0.126621            0.067391    0.347732   \n",
      "\n",
      "   inevitable_war1  inevitable_war2  violence_justification    martyr  \\\n",
      "0         0.164337         0.216782                0.048596  0.008442   \n",
      "\n",
      "   violent_role_model1  violent_role_model2  hopelessness1  hopelessness2  \n",
      "0             0.008479             0.087422        0.16549       0.079438  \n",
      "\n",
      "Sum of All Categories: 18093885\n",
      "Percentage of Total Tokens: 2.03%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/uufs/chpc.utah.edu/common/home/u1472278/QAnon/partial_results_parler.txt'\n",
    "\n",
    "# Initialize a dictionary to store cumulative counts for each category and total tokens\n",
    "cumulative_counts = {\n",
    "    \"total_tokens\": 0,\n",
    "    \"fusion\": 0,\n",
    "    \"violence\": 0,\n",
    "    \"identification1\": 0,\n",
    "    \"identification2\": 0,\n",
    "    \"slurs\": 0,\n",
    "    \"demonisation\": 0,\n",
    "    \"dehumanisation\": 0,\n",
    "    \"existential_threat\": 0,\n",
    "    \"conspiracy\": 0,\n",
    "    \"inevitable_war1\": 0,\n",
    "    \"inevitable_war2\": 0,\n",
    "    \"violence_justification\": 0,\n",
    "    \"martyr\": 0,\n",
    "    \"violent_role_model1\": 0,\n",
    "    \"violent_role_model2\": 0,\n",
    "    \"hopelessness1\": 0,\n",
    "    \"hopelessness2\": 0\n",
    "}\n",
    "\n",
    "# Regular expressions to match relevant lines\n",
    "token_pattern = re.compile(r\"Total number of tokens:\\s+(\\d+)\")\n",
    "category_pattern = re.compile(r\"(\\w+)\\s+(\\d+)\")\n",
    "\n",
    "# Parse the file and aggregate counts\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Check for total tokens line\n",
    "        token_match = token_pattern.search(line)\n",
    "        if token_match:\n",
    "            cumulative_counts[\"total_tokens\"] += int(token_match.group(1))\n",
    "        \n",
    "        # Check for category lines\n",
    "        category_match = category_pattern.search(line)\n",
    "        if category_match:\n",
    "            category, count = category_match.groups()\n",
    "            if category in cumulative_counts:\n",
    "                cumulative_counts[category] += int(count)\n",
    "\n",
    "# Calculate the percentage for each category based on total tokens\n",
    "total_tokens = cumulative_counts[\"total_tokens\"]\n",
    "percentages = {category: (count / total_tokens) * 100 for category, count in cumulative_counts.items() if category != \"total_tokens\"}\n",
    "\n",
    "# Calculate the sum of all category counts and its percentage of the total tokens\n",
    "sum_of_all_categories = sum([count for category, count in cumulative_counts.items() if category != \"total_tokens\"])\n",
    "percentage_of_total_tokens = (sum_of_all_categories / total_tokens) * 100\n",
    "\n",
    "# Convert cumulative counts and percentages to DataFrames\n",
    "df_cumulative_counts = pd.DataFrame([cumulative_counts])\n",
    "df_percentages = pd.DataFrame([percentages])\n",
    "\n",
    "# Display the cumulative counts, percentages, and summary of all categories\n",
    "print(\"Aggregated Counts:\\n\", df_cumulative_counts)\n",
    "print(\"\\nCategory Percentages by Total Tokens:\\n\", df_percentages)\n",
    "print(\"\\nSum of All Categories:\", sum_of_all_categories)\n",
    "print(f\"Percentage of Total Tokens: {percentage_of_total_tokens:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fusion</th>\n",
       "      <th>violence</th>\n",
       "      <th>identification1</th>\n",
       "      <th>identification2</th>\n",
       "      <th>slurs</th>\n",
       "      <th>demonisation</th>\n",
       "      <th>dehumanisation</th>\n",
       "      <th>existential_threat</th>\n",
       "      <th>conspiracy</th>\n",
       "      <th>inevitable_war1</th>\n",
       "      <th>inevitable_war2</th>\n",
       "      <th>violence_justification</th>\n",
       "      <th>martyr</th>\n",
       "      <th>violent_role_model1</th>\n",
       "      <th>violent_role_model2</th>\n",
       "      <th>hopelessness1</th>\n",
       "      <th>hopelessness2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136499</td>\n",
       "      <td>0.20382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050549</td>\n",
       "      <td>0.320724</td>\n",
       "      <td>0.126621</td>\n",
       "      <td>0.067391</td>\n",
       "      <td>0.347732</td>\n",
       "      <td>0.164337</td>\n",
       "      <td>0.216782</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.087422</td>\n",
       "      <td>0.16549</td>\n",
       "      <td>0.079438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fusion  violence  identification1  identification2     slurs  \\\n",
       "0  0.136499   0.20382              0.0              0.0  0.050549   \n",
       "\n",
       "   demonisation  dehumanisation  existential_threat  conspiracy  \\\n",
       "0      0.320724        0.126621            0.067391    0.347732   \n",
       "\n",
       "   inevitable_war1  inevitable_war2  violence_justification    martyr  \\\n",
       "0         0.164337         0.216782                0.048596  0.008442   \n",
       "\n",
       "   violent_role_model1  violent_role_model2  hopelessness1  hopelessness2  \n",
       "0             0.008479             0.087422        0.16549       0.079438  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dotwin aggre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Category Counts:\n",
      "    demonisation  hopelessness2  conspiracy  violence  hopelessness1  fusion  \\\n",
      "0         56951          28159       67476     54416          44848   29000   \n",
      "\n",
      "   violent_role_model2  inevitable_war2  identification2  \\\n",
      "0                20946            89806                0   \n",
      "\n",
      "   violence_justification  identification1  inevitable_war1  \\\n",
      "0                   10364                0            41764   \n",
      "\n",
      "   existential_threat  violent_role_model1  martyr  slurs  dehumanisation  \n",
      "0               19275                 3128    2742  13894           25181  \n",
      "\n",
      "Combined Category Percentages by Total Tokens:\n",
      "    demonisation  hopelessness2  conspiracy  violence  hopelessness1    fusion  \\\n",
      "0      0.222572       0.110049    0.263705  0.212665       0.175272  0.113336   \n",
      "\n",
      "   violent_role_model2  inevitable_war2  identification2  \\\n",
      "0              0.08186         0.350974              0.0   \n",
      "\n",
      "   violence_justification  identification1  inevitable_war1  \\\n",
      "0                0.040504              0.0         0.163219   \n",
      "\n",
      "   existential_threat  violent_role_model1    martyr   slurs  dehumanisation  \n",
      "0            0.075329             0.012225  0.010716  0.0543        0.098411  \n",
      "\n",
      "Sum of All Categories: 507950\n",
      "Percentage of Total Tokens: 1.99%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = '/uufs/chpc.utah.edu/common/home/u1472278/QAnon/partial_results_dotwin.txt'\n",
    "\n",
    "# Regular expressions to capture sections and values\n",
    "total_tokens_pattern = re.compile(r\"Total number of tokens from (\\w+.*\\w+):\\s+(\\d+)\")\n",
    "category_counts_pattern = re.compile(r\"(\\w+)\\s+(\\d+)\")\n",
    "\n",
    "# Dictionary to store token counts and category counts for each section\n",
    "data = {\n",
    "    \"title_content\": {\n",
    "        \"total_tokens\": 0,\n",
    "        \"counts\": {}\n",
    "    },\n",
    "    \"content\": {\n",
    "        \"total_tokens\": 0,\n",
    "        \"counts\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Parse the file\n",
    "current_section = None\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Match the total tokens line\n",
    "        tokens_match = total_tokens_pattern.search(line)\n",
    "        if tokens_match:\n",
    "            section, tokens = tokens_match.groups()\n",
    "            current_section = \"title_content\" if \"title\" in section else \"content\"\n",
    "            data[current_section][\"total_tokens\"] = int(tokens)\n",
    "        \n",
    "        # Match each category count\n",
    "        category_match = category_counts_pattern.search(line)\n",
    "        if category_match and current_section:\n",
    "            category, count = category_match.groups()\n",
    "            data[current_section][\"counts\"][category] = int(count)\n",
    "\n",
    "# Aggregating combined totals for each category\n",
    "combined_total_tokens = data[\"title_content\"][\"total_tokens\"] + data[\"content\"][\"total_tokens\"]\n",
    "combined_counts = {\n",
    "    category: data[\"title_content\"][\"counts\"].get(category, 0) + data[\"content\"][\"counts\"].get(category, 0)\n",
    "    for category in set(data[\"title_content\"][\"counts\"]) | set(data[\"content\"][\"counts\"])\n",
    "}\n",
    "\n",
    "# Calculate the sum of all combined categories and its percentage of the combined total tokens\n",
    "sum_combined_counts = sum(combined_counts.values())\n",
    "percentage_combined_total = (sum_combined_counts / combined_total_tokens) * 100\n",
    "\n",
    "# Calculate percentage for each category based on combined total tokens\n",
    "percentages_combined = {category: (count / combined_total_tokens) * 100 for category, count in combined_counts.items()}\n",
    "\n",
    "# Convert counts and percentages to DataFrames for display\n",
    "df_combined_counts = pd.DataFrame([combined_counts])\n",
    "df_combined_percentages = pd.DataFrame([percentages_combined])\n",
    "\n",
    "# Display results\n",
    "print(\"Combined Category Counts:\\n\", df_combined_counts)\n",
    "print(\"\\nCombined Category Percentages by Total Tokens:\\n\", df_combined_percentages)\n",
    "print(\"\\nSum of All Categories:\", sum_combined_counts)\n",
    "print(f\"Percentage of Total Tokens: {percentage_combined_total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
